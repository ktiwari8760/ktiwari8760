#NUMBERS - 

#Division - 
7/4  O/P - 1.75
7//4 O/P - 1(FLOOR DIVISION answer is Quotient)
7%4  O/P - 3 (MODULO, PROVIDES REMAINDER)

# Order of Operations followed in Python
2 + 10 * 10 + 3 --> 105

# Can use parentheses to specify orders
(2+10) * (10+3) --> 156

--------------------------------------------------------------------------------------------

#STRINGS


#LEN FUNCTION: 
len('Hello World') --> 11

# STRING INDEXING 
s = 'Hello World'
s[0] = 'H'
s[2] = 'l'

# Grab everything past the first term all the way to the length of s which is len(s)
s[1:] = 'ello World'

# Grab everything UP TO the 3rd index
s[:3] = 'Hel' # or s[0:3] will give same output.

# Grab Everything
s[:] = 'Hello World'

# Last letter (one index behind 0 so it loops back around)
s[-1] = 'd'

# Grab everything but the last letter
s[:-1] = 'Hello Worl'

# Grab everything, but go in step sizes of 2
s[::2] = 'HloWrd'

# We can use this to print a string backwards (REVERSE OF STRING)
s[::-1] = 'dlroW olleH'


# We can reassign s completely though!
s = s + ' concatenate me!'
print(s) --> 'Hello World concatenate me!'

#Concatenate by Multiplying
letter = 'z'
letter*5 --> 'zzzzz'

#BUILT IN STRING METHODS

s.upper(), s.lower()

# Split a string by blank space (this is the default)
s.split() --> ['Hello', 'World', 'concatenate', 'me!']

# Split by a specific element (doesn't include the element that was split on)
s.split('W') --> ['Hello ', 'orld concatenate me!']

#Print Formatting. We can use the .format() method to add formatted objects to printed string statements. 

print('Insert another string with curly brackets: {}'.format('The inserted string'))

O/p --> 'Insert another string with curly brackets: The inserted string'




--------------------------------------------------------------------------------------------------------------------------------

LISTS -->

#Lists can be thought of the most general version of a sequence in Python. Unlike strings, they are mutable, meaning the elements inside a list 
#can be changed!

# Assign a list to an variable named my_list
my_list = [1,2,3]

my_list = ['A string',23,100.232,'o']

len(my_list) --> 4

#INDEXING AND SLICING OF LISTS:
-------------------------------

my_list = ['one','two','three',4,5]

my_list[0] --> 'one'

# Grab index 1 and everything past it
my_list[1:] --> ['two','three',4,5]

# Grab everything UP TO index 3, not inclusive of 3.
my_list[:3] --> ['one', 'two', 'three']

We can also use + to concatenate lists, just like we did for strings.
my_list + ['new item'] --> ['one', 'two', 'three', 4, 5, 'new item']

NOTE** --> Here if you need to add the above item you need to re-assign the list like below to make the change permanent or else above statement is 
temporal.

# Reassign
my_list = my_list + ['add new item permanently']
my_list --> ['one', 'two', 'three', 4, 5, 'add new item permanently']

# Make the list double
my_list * 2 --> ['one','two','three',4,5,'add new item permanently','one','two','three',4,5,'add new item permanently']

# Again doubling not permanent, Re-assignment must be done.
my_list --> ['one', 'two', 'three', 4, 5, 'add new item permanently']


LIST METHODS --
-----------------

# Create a new list
list1 = [1,2,3]

# Appends ITEM PERMANENTLY
list1.append('append me!')

NOTE --> By Append methods always add the element at last. Append can add any of the datatypes whether it be dictionary,tuple/list.
Append will add but will not return any value in the output Window.

# Pop /Remove off the 0 indexed item
list1.pop(0) --> [2, 3, 'append me!']
new_list = ['a','e','x','b','c']

NOTE--> By default pop() method removes last index element and Returns the removed element value in the output.

new_lst.remove('b')  #'b' will be removed from new_lst list.
NOTE --> The remove() method removes the first occurrence of the element with the specified value.
Value is mandatory needed for remove method, else it throws an error. Does not returns any value in the output window.
TypeError: remove() takes exactly one argument (0 given)


Clear Method:
new_lst.clear()  # removes all elements from the list. List will become empty. Nothing will be returned.


Copy Method:
The copy() method returns a copy of the specified list.

new_lst = lst.copy()  # it copies the elements of the list.


Count Method:
The count() method returns the number of elements with the specified value.

points = [1, 4, 2, 9, 7, 8, 9, 3, 1]
x = points.count(9)
x
>>> 2


EXTEND METHOD:
The extend() method adds the specified list elements (or any iterable) to the end of the current list.

fruits = ['apple', 'banana', 'cherry']
cars = ['Ford', 'BMW', 'Volvo']
fruits.extend(cars)

>>> ['apple', 'banana', 'cherry','Ford', 'BMW', 'Volvo']


fruits = ['apple', 'banana', 'cherry']
cars = ['Ford', 'BMW', 'Volvo']
fruits.append(cars)
print(fruits)

>>> ['apple', 'banana', 'cherry', ['Ford', 'BMW', 'Volvo']]


INDEX METHOD:
-------------
The index() method returns the position at the first occurrence of the specified value.

fruits = [4, 55, 64, 32, 16, 32]
x = fruits.index(32)

>>> 3

Note: The index() method only returns the first occurrence of the value.

INSERT METHOD:
--------------
The insert() method inserts the specified value at the specified position.

fruits = ['apple', 'banana', 'cherry']
fruits.insert(1, "orange")

>>> ['apple', 'orange', 'banana', 'cherry']



# Use reverse to reverse order (this is permanent!)
new_list.reverse() --> ['c', 'b', 'x', 'e', 'a']

# Use sort to sort the list (in this case alphabetical order, but for numbers it will go ascending)
new_list.sort() --> ['a', 'b', 'c', 'e', 'x']
The sort() method sorts the list ascending by default.

You can also make a function to decide the sorting criteria(s).
cars = ['Ford', 'BMW', 'Volvo']
cars.sort(reverse=True)
cars

>>> ['Volvo', 'Ford', 'BMW']



#Nesting Lists -->
#Python data structures supports nesting. This means we can have data structures within data structures.
# Let's make three lists
lst_1=[1,2,3]
lst_2=[4,5,6]
lst_3=[7,8,9]

# Make a list of lists to form a matrix
matrix = [lst_1,lst_2,lst_3] --> [[1, 2, 3], [4, 5, 6], [7, 8, 9]]

# Grab first item in matrix object
matrix[0] --> [1, 2, 3]
matrix[0][0] --> 1

# Build a list comprehension by deconstructing a for loop within a []
first_col = [row[0] for row in matrix] --> [1,4,7] (returning first column of matrix)
-----------------------------------------------------------------------------------------------------------------------------------

DICTIONARIES
-------------

Something about mappings in Python. In other languages you can think of Dictionaries as hash tables.
So what are mappings? Mappings are a collection of objects that are stored by a key, unlike a sequence that stored objects by their relative position. 
This is an important distinction, since mappings won't retain order since they have objects defined by a key.A Python dictionary consists of a key 
and then an associated value.

# Make a dictionary with {} and : to signify a key and a value
my_dict = {'key1':'value1','key2':'value2'}

# Call values by their key
my_dict['key2'] --> 'value2'

#flexible in datatypes and can store any type of data within.
my_dict = {'key1':123,'key2':[12,23,33],'key3':['item0','item1','item2']}

# Let's call items from the dictionary
my_dict['key3'] --> ['item0', 'item1', 'item2']

# Can call any index on that value
my_dict['key3'][0] --> 'item0'

# Can then even call methods on that value
my_dict['key3'][0].upper()

# Subtract 123 from the value
my_dict['key1'] --> 123
my_dict['key1'] = my_dict['key1'] - 123 --> 0

#Create a new dictionary by assignment manually -->
# Create a new dictionary
d = {}
d['animal'] = 'Dog'
d['answer'] = 42
d --> {'animal': 'Dog', 'answer': 42}

Nesting with Dictionaries --
-----------------------------

# Dictionary nested inside a dictionary nested inside a dictionary
d = {'key1':{'nestkey':{'subnestkey':'value'}}}

# Keep calling the keys
d['key1']['nestkey']['subnestkey'] --> 'value'


DICTIONARY METHODS:
-------------------

# Create a typical dictionary
d = {'key1':1,'key2':2,'key3':3}

# Method to return a list of all keys 
d.keys() --> dict_keys(['key1', 'key2', 'key3'])
d.values() --> dict_values([1, 2, 3])

# Method to return tuples of all items  (we'll learn about tuples soon)
d.items() --> return dictionary as a tuple in output.

----------------------------------------------------------------------------------------------------------------------------------------


Tuples --
----------

In Python tuples are very similar to lists, however, unlike lists they are immutable meaning they can not be changed. You would use tuples to 
present things that shouldn't be changed, such as days of the week, or dates on a calendar.

# Create a tuple
t = (1,2,3)

# Can also mix object types
t = ('one',2)

# Use indexing just like we did in lists
t[0] --> 'one'

# Slicing just like a list
t[-1] --> 2

TUPLE METHODS --
------------------

# Use .index to enter a value and return the index
t.index('one') --> 0

# Use .count to count the number of times a value appears
t.count('one') --> 1

NO RE-ASSIGNMENT OR APPEND METHOD WORKS FOR TUPLES SINCE THEY ARE IMMUTABLE.

When to use Tuples?

You may be wondering, "Why bother using tuples when they have fewer available methods?" To be honest, tuples are not used as often as lists in 
programming, but are used when immutability is necessary. If in your program you are passing around an object and need to make sure it does not get 
changed, then a tuple becomes your solution. It provides a convenient source of data integrity.

------------------------------------------------------------------------------------------------------------------------------------------

#Sets & BOOLEANS-->
----------------------
#Sets are an unordered collection of "unique" elements. We can construct them by using the set() function.

x = set()
# We add to sets with the add() method
x.add(1) --> {1}

# Add a different element
x.add(2) --> {1,2}

# Create a list with repeats
list1 = [1,1,2,2,3,4,5,6,1,1]
# Cast as set to get unique values
set(list1) --> {1,2,3,4,5,6}

#BOOLEANS --
----------

Python comes with Booleans (with predefined True and False displays that are basically just the integers 1 and 0). It also has a placeholder object 
called None.

# Set object to be a boolean
a = True
# Output is boolean
1 > 2 --> False

# None placeholder
b = None
print(b) --> None.
--------------------------------------------------------------------------------------------------------------------------------------------------

FILES --
---------

Python uses file objects to interact with external files on your computer. These file objects can be any sort of file you have on your computer, 
whether it be an audio file, a text file, emails, Excel documents, etc.Python has a built-in open function that allows us to open and play
 with basic file types.

Python Opening a file:
-----------------------
myfile = open('test.txt')
#CHECK FOR THE CORRECT PATH BEFORE RUNNING ABOVE STATEMENT OR ELSE IT WILL THROW FILE NOT FOUND ERROR.

# We can now read the file
my_file.read()

# But what happens if we try to read it again?
my_file.read() --> ''
This happens because you can imagine the reading "cursor" is at the end of the file after having read it. So there is nothing left to read. We can 
reset the "cursor" like this:
# Seek to the start of file (index 0)
my_file.seek(0)

# Now read again
my_file.read() --> 'Hello, this is a quick test file.'
my_file.close()

#By default, the open() function will only allow us to read the file. We need to pass the argument 'w' to write over the file. For example:
# Add a second argument to the function, 'w' which stands for write.
# Passing 'w+' lets us read and write to the file

my_file = open('test.txt','w+') 
#NOTE** (Opening a file with 'w' or 'w+' truncates the original, meaning that anything that was in the original file is deleted!)

# Write to the file
my_file.write('This is a new line')
# Read the file
my_file.seek(0)
my_file.read() --> 'This is a new line'
my_file.close()  # always do this when you're done with a file

APPENDING a file:
--------------------
Passing the argument 'a' opens the file and puts the pointer at the end, so anything written is appended. Like 'w+', 'a+' lets us read and write 
to a file. If the file does not exist, one will be created.

my_file = open('test.txt','a+')
my_file.write('\nThis is text being appended to test.txt')
my_file.write('\nAnd another line here.')

my_file.seek(0)
print(my_file.read()) --> 
This is a new line
This is text being appended to test.txt
And another line here.

my_file.close()


ITERATING THROUGH A FILE --
---------------------------

for line in open('test.txt'):
    print(line)

	
O/P -->
FIRST LINE

SECOND LINE


----------------------------------------------------------------------------------------------------------------------------------------------------------

LOOPS -->
-------------

#IF - ELIF - ELSE
-----------------
person = 'George'

if person == 'Sammy':
    print('Welcome Sammy!')
elif person =='George':
    print('Welcome George!')
else:
    print("Welcome, what's your name?")


>>> Welcome George!

--------------------------------------------

person = 'Sammy'

if person == 'Sammy':
    print('Welcome Sammy!')
else:
    print("Welcome, what's your name?")
	
>>> Welcome Sammy!


---------------------------------------------

FOR LOOPS :
-----------

# We'll learn how to automate this sort of list in the next lecture
list1 = [1,2,3,4]
for num in list1:
    print(num)

>>> 1
	2
	3
	4
	
#EVEN NUMBERS
for num in list1:
    if num % 2 == 0:
        print(num)

>>> 2
	4

#EVEN OR ELSE PRINT ODD NUMBER
for num in list1:
    if num % 2 == 0:
        print(num)
    else:
        print('Odd number')
		
>>> Odd number
	2
	Odd number
	4

#string example
for letter in 'This':
    print(letter)

>>> T
	h
	i
	s


#LIST EXAMPLE
list2 = [(2,4),(6,8),(10,12)]
for tup in list2:
    print(tup)

>>>
(2, 4)
(6, 8)
(10, 12)

# Now with unpacking!
for (t1,t2) in list2:
    print(t1)

>>>
2
6
10

#dictionaries
d = {'k1':1,'k2':2,'k3':3}
for item in d:
    print(item)

>>>
k1
k2
k3


# WHILE LOOPS :
A while statement will repeatedly execute a single statement or group of statements as long as the condition is true. 
The reason it is called a 'loop' is because the code statements are looped through over and over again until the condition is no longer met.

x = 0
while x < 4:
    print('x is currently: ',x)
    print(' x is still less than 10, adding 1 to x')
    x+=1

>>>
x is currently:  0
 x is still less than 10, adding 1 to x
x is currently:  1
 x is still less than 10, adding 1 to x
x is currently:  2
 x is still less than 10, adding 1 to x
x is currently:  3
 x is still less than 10, adding 1 to x

#EXAMPLE 2
x = 0
while x < 4:
    print('x is currently: ',x)
    print(' x is still less than 10, adding 1 to x')
    x+=1
    
else:
    print('All Done!')

>>>
x is currently:  0
 x is still less than 10, adding 1 to x
x is currently:  1
 x is still less than 10, adding 1 to x
x is currently:  2
 x is still less than 10, adding 1 to x
x is currently:  3
 x is still less than 10, adding 1 to x
All Done!


#BREAK, CONTINUE, PASS
break: Breaks out of the current closest enclosing loop.
continue: Goes to the top of the closest enclosing loop.
pass: Does nothing at all.

#EXAMPLE 1:
x = 0
while x < 4:
    print('x is currently: ',x)
    print(' x is still less than 10, adding 1 to x')
    x+=1
    if x==3:
        print('Breaking because x==3')
        break
    else:
        print('continuing...')
        continue
>>>
x is currently:  0
 x is still less than 10, adding 1 to x
continuing...
x is currently:  1
 x is still less than 10, adding 1 to x
continuing...
x is currently:  2
 x is still less than 10, adding 1 to x
Breaking because x==3


LIST COMPREHENSIONS : 
-----------------------
#it is essentially a one line for loop built inside of brackets


# Grab every letter in string
lst = [x for x in 'word'] --> ['w','o','r','d']

# Square numbers in range and turn into list
lst = [x**2 for x in range(0,11)] --> [0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100]

# Check for even numbers in a range
lst = [x for x in range(11) if x % 2 == 0] --> [0, 2, 4, 6, 8, 10]

# Convert Celsius to Fahrenheit
celsius = [0,10,20.1,34.5]
fahrenheit = [((9/5)*temp + 32) for temp in celsius ]
fahrenheit --> [32.0, 50.0, 68.18, 94.1]


#NESTED LIST OPERATION:
lst = [ x**2 for x in [x**2 for x in range(11)]]
lst --> [0, 1, 16, 81, 256, 625, 1296, 2401, 4096, 6561, 10000]

========================================================================================================================================================

FUNCTIONS :
----------

#example for checking if a number is PRIME:
def is_prime(num):
    '''
    Naive method of checking for primes. 
    '''
    for n in range(2,num):
        if num % n == 0:
            print(num,'is not prime')
            break
    else: # If never mod zero, then prime
        print(num,'is prime!')

#Write a function that returns the lesser of two given numbers if both numbers are even, but returns the greater if one or both numbers are odd.
def lesser_of_two_evens(a,b):
    if a%2 == 0 and b%2 == 0:
        return min(a,b)
    else:
        return max(a,b)
		

#Write a function takes a two-word string and returns True if both words begin with same letter.
def animal_crackers(text):
    wordlist = text.split()
    return wordlist[0][0] == wordlist[1][0]

#Given two integers, return True if the sum of the integers is 20 or if one of the integers is 20. If not, return False
def makes_twenty(n1,n2):
    return (n1+n2)==20 or n1==20 or n2==20
	

#Write a function that capitalizes the first and fourth letters of a name.
def old_macdonald(name):
    if len(name) > 3:
        return name[:3].capitalize() + name[3:].capitalize()
    else:
        return 'Name is too short!'

# Check
old_macdonald('macdonald') --> 'MacDonald'

#Given a sentence, return a sentence with the words reversed.
def master_yoda(text):
    return ' '.join(text.split()[::-1])

master_yoda('I am home') --> 'home am I'


#Given three integers between 1 and 11, if their sum is less than or equal to 21, return their sum. If their sum exceeds 21 and there's an eleven, reduce the total sum by 10. Finally, if the sum (even after adjustment) exceeds 21, return 'BUST'.
def blackjack(a,b,c):
    
    if sum((a,b,c)) <= 21:
        return sum((a,b,c))
    elif sum((a,b,c)) <=31 and 11 in (a,b,c):
        return sum((a,b,c)) - 10
    else:
        return 'BUST'

#Write a function that returns the number of prime numbers that exist up to and including a given number.
def count_primes(num):
    primes = [2]
    x = 3
    if num < 2:  # for the case of num = 0 or 1
        return 0
    while x <= num:
        for y in range(3,x,2):  # test all odd factors up to x-1
            if x%y == 0:
                x += 2
                break
        else:
            primes.append(x)
            x += 2
    print(primes)
    return len(primes)

# Check
count_primes(100) -> [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]

---------------------------------------------------------------------------------------------------------------------------------------------------------

LAMBDA EXPRESSIONS:
-------------------

#MAP FUNCTION -
The map function allows you to map/use a function to an iterable object.

eg1) 
def square(num):    #function
    return num**2


my_nums = [1,2,3,4,5] #iterable object
map(square,my_nums)   #mapping square function to my_nums.

# To get the results, either iterate through map() 
# or just cast to a list
list(map(square,my_nums)) --> [1, 4, 9, 16, 25]

Eg2)
def splicer(mystring):
    if len(mystring) % 2 == 0:
        return 'even'
    else:
        return mystring[0]
		
mynames = ['John','Cindy','Sarah','Kelly','Mike']
list(map(splicer,mynames))

>>> ['even', 'C', 'S', 'K', 'even']



# FILTER FUNCTION :
The filter function returns an iterator yielding those items of iterable for which function(item) is true. Meaning you need to filter by a function 
that returns either True or False. Then passing that into filter (along with your iterable) and you will get back only the results that would return 
True when passed to the function.

def check_even(num):
    return num % 2 == 0

nums = [0,1,2,3,4,5,6,7,8,9,10]
list(filter(check_even,nums))

>>> [0, 2, 4, 6, 8, 10]


# LAMBDA EXPRESSION:
--------------------

Lambda expressions allow us to create "anonymous" functions. This basically means we can quickly make ad-hoc functions without needing to properly 
define a function using def.

EXAMPLE1) 
def square(num):
    return num**2


lambda num: num ** 2

#Let's repeat some of the examples from above with a lambda expression.
list(map(lambda num: num ** 2, my_nums)) --> [1, 4, 9, 16, 25]

list(filter(lambda n: n % 2 == 0,nums)) --> [0, 2, 4, 6, 8, 10]

#Lambda expression for grabbing the first character of a string: 
lambda s: s[0]

#Lambda expression for reversing a string: 
lambda s: s[::-1]

#multiple arguments can be passed.
lambda x,y : x + y

--------------------------------------------------------------------------------------------------------------------------------------------------------------

*args --
----------
When a function parameter starts with an asterisk, it allows for an arbitrary number of arguments, and the function takes them in as a tuple of 
values. Rewriting the above function:

def myfunc(*args):
    return sum(args)*.05

myfunc(40,60,20) --> 6.0

**kwargs
--------
Similarly, Python offers a way to handle arbitrary numbers of keyworded arguments. Instead of creating a tuple of values, 
**kwargs builds a dictionary of key/value pairs. For example:

def myfunc(**kwargs):
    if 'fruit' in kwargs:
        print(f"My favorite fruit is {kwargs['fruit']}")  # review String Formatting and f-strings if this syntax is unfamiliar
    else:
        print("I don't like fruit")
>>>        
myfunc(fruit='pineapple') --> My favorite fruit is pineapple
myfunc()--> I don't like fruit


def myfunc(*args, **kwargs):
    if 'fruit' and 'juice' in kwargs:
        print(f"I like {' and '.join(args)} and my favorite fruit is {kwargs['fruit']}")
        print(f"May I have some {kwargs['juice']} juice?")
    else:
        pass
>>>        
myfunc('eggs','spam',fruit='cherries',juice='orange')
I like eggs and spam and my favorite fruit is cherries
May I have some orange juice?


#Write a Python function that accepts a string and calculates the number of upper case letters and lower case letters.
def up_low(s):
    d={"upper":0, "lower":0}
    for c in s:
        if c.isupper():
            d["upper"]+=1
        elif c.islower():
            d["lower"]+=1
        else:
            pass
    print("Original String : ", s)
    print("No. of Upper case characters : ", d["upper"])
    print("No. of Lower case Characters : ", d["lower"])

>>>
s = 'Hello Mr. Rogers, how are you this fine Tuesday?'
up_low(s)
Original String :  Hello Mr. Rogers, how are you this fine Tuesday?
No. of Upper case characters :  4
No. of Lower case Characters :  33

-------------------------------------------------------------------------------------------------------------------------------------------------------------

OOPS CONCEPTS --
-------------

class Circle():  #CLASS NAME SHOULD BE CAMEL CASING ALWAYS.
    pi = 3.14  # GLOBAL ATRRIBUTE and will remain same for any instance of the Class

    # Circle gets instantiated with a radius (default is 1)
    def __init__(self, radius=1):
        self.radius = radius 
        self.area = radius * radius * Circle.pi  #Global attribute pi can also be referred as "self.pi or Circle.pi"

    # Method for resetting Radius
    def setRadius(self, new_radius):
        self.radius = new_radius
        self.area = new_radius * new_radius * self.pi

    # Method for getting Circumference
    def getCircumference(self):
        return self.radius * self.pi * 2


c = Circle()  #Object of the Class Circle

print('Radius is: ',c.radius)
print('Area is: ',c.area)
print('Circumference is: ',c.getCircumference())


>>>
Radius is:  1
Area is:  3.14
Circumference is:  6.28

------------------------------------------------------------------------------------------------------------------------

INHERITANCE --
-----------

Inheritance is a way to form new classes using classes that have already been defined. The newly formed classes are called derived classes, 
the classes that we derive from are called base classes. Important benefits of inheritance are code reuse and reduction of complexity of a program.
 The derived classes (descendants) override or extend the functionality of base classes (ancestors).

class Animal():
    def __init__(self):
        print("Animal created")

    def whoAmI(self):
        print("Animal")

    def eat(self):
        print("Eating")


class Dog(Animal):					#Derived Class
    def __init__(self):
        Animal.__init__(self)
        print("Dog created")

    def whoAmI(self):
        print("Dog")

    def bark(self):
        print("Woof!")


d = Dog() --> Animal created
			  Dog created

d.whoAmI() --> Dog

d.eat()    --> Eating

d.bark()   --> Woof!


In this example, we have two classes: Animal and Dog. The Animal is the base class, the Dog is the derived class.
The derived class inherits the functionality of the base class. 
•It is shown by the eat() method. 

The derived class modifies existing behavior of the base class.
•shown by the whoAmI() method. 

Finally, the derived class extends the functionality of the base class, by defining a new bark() method.

--------------------------------------------------------------------------------------------------------------------------------------------

POLYMORPHISM --
-------------

We've learned that while functions can take in different arguments, methods belong to the objects they act on. 
In Python, polymorphism refers to the way in which different object classes can share the same method name, and those methods can be called from the
same place even though a variety of different objects might be passed in. The best way to explain this is by example:

class Dog():
    def __init__(self, name):
        self.name = name

    def speak(self):
        return self.name+' says Woof!'
    
class Cat():
    def __init__(self, name):
        self.name = name

    def speak(self):
        return self.name+' says Meow!' 
    
niko = Dog('Niko')
felix = Cat('Felix')

print(niko.speak())
print(felix.speak())

>>>
Niko says Woof!
Felix says Meow!


Here we have a Dog class and a Cat class, and each has a .speak() method. When called, each object's .speak() method returns a result unique to the 
object.There a few different ways to demonstrate polymorphism. First, with a for loop:

1) 
for pet in [niko,felix]:
    print(pet.speak())

>>>
Niko says Woof!
Felix says Meow!

#WITH THE HELP OF FUNCTIONS:
2)
def pet_speak(pet):
    print(pet.speak())

pet_speak(niko)
pet_speak(felix)

>>>
Niko says Woof!
Felix says Meow!


In both cases we were able to pass in different object types, and we obtained object-specific results from the same mechanism.
A more common practice is to use abstract classes and inheritance. An abstract class is one that never expects to be instantiated. For example,
we will never have an Animal object, only Dog and Cat objects, although Dogs and Cats are derived from Animals:

class Animal():
    def __init__(self, name):    # Constructor of the class
        self.name = name

    def speak(self):              # Abstract method, defined by convention only
        raise NotImplementedError("Subclass must implement abstract method")


class Dog(Animal):
    
    def speak(self):
        return self.name+' says Woof!'
    
class Cat(Animal):

    def speak(self):
        return self.name+' says Meow!'
    
fido = Dog('Fido')
isis = Cat('Isis')

print(fido.speak())
print(isis.speak())

>>>
Fido says Woof!
Isis says Meow!


NOTE: Real life examples of polymorphism include:
•opening different file types - different tools are needed to display Word, pdf and Excel files
•adding different objects - the + operator performs arithmetic and concatenation

-----------------------------------------------------------------------------------------------------------------------------------------------

EXCEPTIONS HANDLING --
--------------------

SYNTACTICAL BLOCK(BELOW)

try:
   You do your operations here...
   ...
except ExceptionI:
   If there is ExceptionI, then execute this block.
except ExceptionII:
   If there is ExceptionII, then execute this block.
   ...
else:
   If there is no exception then execute this block. 


EXAMPLE1) 
try:
    f = open('testfile','w')
    f.write('Test write this')
except IOError:
    # This will only check for an IOError exception and then execute this print statement
    print("Error: Could not find file or read data")
else:
    print("Content written successfully")
    f.close()
	
>>>
Content written successfully


try:
    f = open('testfile','r')			#Files permission changed to read only.
    f.write('Test write this')
except IOError:
    # This will only check for an IOError exception and then execute this print statement
    print("Error: Could not find file or read data")
else:
    print("Content written successfully")
    f.close()

>>>
"Error: Could not find file or read data"


try:
    f = open('testfile','r')
    f.write('Test write this')
except:
    # This will check for any exception and then execute this print statement
    print("Error: Could not find file or read data")
else:
    print("Content written successfully")
    f.close()

>>>
"Error: Could not find file or read data"


FINALLY :
--------

The finally: block of code will always run regardless if there was an exception in the try code block.

def askint():
    try:
        val = int(input("Please enter an integer: "))
    except:
        print("Looks like you did not enter an integer!")

    finally:
        print("Finally, I executed!")
    print(val)

askint()

>>>
Please enter an integer: 5
Finally, I executed!
5


def askint():
    while True:
        try:
            val = int(input("Please enter an integer: "))
        except:
            print("Looks like you did not enter an integer!")
            continue
        else:
            print("Yep that's an integer!")
            break
        finally:
            print("Finally, I executed!")
        print(val)

askint()

>>>
Please enter an integer: five
Looks like you did not enter an integer!
Finally, I executed!
Please enter an integer: four
Looks like you did not enter an integer!
Finally, I executed!
Please enter an integer: 3
Yep that's an integer!
Finally, I executed!

NOTICE ABOVE** - So why did our function print "Finally, I executed!" after each trial, yet it never printed val itself? This is because with a 
try/except/finally clause, any continue or break statements are reserved until after the try clause is completed. 
This means that even though a successful input of 3 brought us to the else: block, and a break statement was thrown, the try clause continued 
through to finally: before breaking out of the while loop. And since print(val) was outside the try clause, the break statement prevented it 
from running.


LET'S MAKE ONE MORE CHANGE - 

def askint():
    while True:
        try:
            val = int(input("Please enter an integer: "))
        except:
            print("Looks like you did not enter an integer!")
            continue
        else:
            print("Yep that's an integer!")
            print(val)
            break
        finally:
            print("Finally, I executed!")

askint()

>>>
Please enter an integer: six
Looks like you did not enter an integer!
Finally, I executed!
Please enter an integer: 6
Yep that's an integer!
6
Finally, I executed!


---------------------------------------------------------------------------------------------------------------------------------------------------------

NUMPY Introduction --
-------------------

Numpy Arrays
------------
NumPy arrays are the main way we will use Numpy throughout the course. Numpy arrays essentially come in two flavors: vectors and matrices. 
Vectors are strictly 1-d arrays and matrices are 2-d( but a 1d matric also exist).

List to Array Conversion -
-----------------------
import numpy as np

my_matrix = [[1,2,3],[4,5,6],[7,8,9]]
my_matrix   ---> [[1, 2, 3], [4, 5, 6], [7, 8, 9]]

np.array(my_matrix)  ---> array([[1, 2, 3],
								[4, 5, 6],
								[7, 8, 9]])
								
								

Built-in Methods :
----------------

np.arange(0,10)
>>> array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

np.arange(0,11,2)
>>> array([ 0,  2,  4,  6,  8, 10])

np.zeros(3)
>>> array([ 0.,  0.,  0.])

np.zeros((5,5))
>>>
array([[ 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.],
       [ 0.,  0.,  0.,  0.,  0.]])
	   
	   
np.ones((3,3))
>>> 
array([[ 1.,  1.,  1.],
       [ 1.,  1.,  1.],
       [ 1.,  1.,  1.]])
	   

Linspace:
---------
np.linspace(0,10,3)
>>>
array([  0.,   5.,  10.])

IDENTITY MATRIX:
np.eye(4)
>>>
array([[ 1.,  0.,  0.,  0.],
       [ 0.,  1.,  0.,  0.],
       [ 0.,  0.,  1.,  0.],
       [ 0.,  0.,  0.,  1.]])



Random Method:
--------------
Numpy has lots of ways to create random number arrays:


rand:
Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1).

np.random.rand(2)
>>>
array([ 0.11570539,  0.35279769])


np.random.rand(5,2)
>>>
array([[0.2422364 , 0.9341005 ],
       [0.3824845 , 0.65821232],
       [0.99226435, 0.99017696],
       [0.22981969, 0.24052848],
       [0.46046125, 0.87958096]])
	   

randn:
Return a sample (or samples) from the "standard normal" distribution. Unlike rand which is uniform:

np.random.randn(2)
>>>
array([-0.27954018,  0.90078368])


np.random.randn(5,2)
>>>
array([[ 0.7300585 ,  0.79031589],
       [-0.16018429,  0.69222727],
       [ 0.11321596,  0.54450969],
       [ 0.31017742, -0.65449077],
       [ 1.35709046, -0.97241062]])
	   

randint(min,max,no.of random values required):
Generate random integers from low (inclusive) to high (exclusive).

np.random.randint(1,100)
>>> 44

np.random.randint(1,100,10)
>>>
array([13, 64, 27, 63, 46, 68, 92, 10, 58, 24])


Array Attributes and Methods:
-----------------------------

arr = np.arange(25)  --> array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24])
ranarr = np.random.randint(0,50,10)  --> array([10, 12, 41, 17, 49, 2,46,3,19, 39])

Reshape:
Returns an array containing the same data with a new shape

arr.reshape(5,5)
>>>
array([[ 0,  1,  2,  3,  4],
       [ 5,  6,  7,  8,  9],
       [10, 11, 12, 13, 14],
       [15, 16, 17, 18, 19],
       [20, 21, 22, 23, 24]])
	   


MAX,MIN,ARGMAX,ARGMIN:
-------------------------
These are useful methods for finding max or min values. Or to find their index locations using argmin or argmax.


ranarr.max() --> 49

ranarr.argmax() --> 4

ranarr.min() --> 2

ranarr.argmin() --> 5


Shape:
Shape is an attribute that arrays have (not a method):

arr.shape # Since its a vector only 1-dim
>>> (25,)

arr.reshape(1,25)
>>>
array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
        17, 18, 19, 20, 21, 22, 23, 24]])

arr.reshape(1,25).shape
>>> (1, 25)

arr.reshape(25,1)
>>>
array([[ 0],
       [ 1],
       [ 2],
       [ 3],
       [ 4],
       [ 5],
       [ 6],
       [ 7],
       [ 8],
       [ 9],
       [10],
       [11],
       [12],
       [13],
       [14],
       [15],
       [16],
       [17],
       [18],
       [19],
       [20],
       [21],
       [22],
       [23],
       [24]])


dtype:
------
You can also grab the data type of the object in the array:

arr.dtype
>>>
dtype('int64')


NUMPY INDEXING AND SELECTION:
-----------------------------

import numpy as np
arr = np.arange(0,11)
arr

>>> array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])


Bracket Indexing and Selection:
-------------------------------

arr[8]  --> 8

arr[1:5] --> array([1, 2, 3, 4])

arr[2:5] --> array([2, 3, 4])


Broadcasting:
------------
Numpy arrays differ from a normal Python list because of their ability to broadcast. If the dimensions of two arrays are dissimilar, 
element-to-element operations are not possible. However, operations on arrays of non-similar shapes is still possible in NumPy, 
because of the broadcasting capability. The smaller array is broadcast to the size of the larger array so that they have compatible shapes.

#Setting a value with index range (Broadcasting)
arr[0:5]=100
arr    --> array([100, 100, 100, 100, 100,   5,   6,   7,   8,   9,  10])

#To get a copy, need to be explicit
arr_copy = arr.copy()
arr_copy --> array([100, 100, 100, 100, 100,   5,   6,   7,   8,   9,  10])


INDEXING a 2D array (matrices):
--------------------------------
The general format is arr_2d[row][col] or arr_2d[row,col]. I recommend usually using the comma notation for clarity.

arr_2d = np.array(([5,10,15],[20,25,30],[35,40,45]))

arr_2d
>>>
array([[ 5, 10, 15],
       [20, 25, 30],
       [35, 40, 45]])

# Format is arr_2d[row][col] or arr_2d[row,col]
# Getting individual element value
arr_2d[1][0] --> 20


#2D array slicing
#Shape (2,2) from top right corner
arr_2d[:2,1:]
>>>
array([[10, 15],
       [25, 30]])
	   

SELECTION:
----------
Let's briefly go over how to use brackets for selection based off of comparison operators.

arr = np.arange(1,11)
arr
>>>  array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])

arr > 4
>>> array([False, False, False, False,  True,  True,  True,  True,  True,  True], dtype=bool)


arr[arr>2]
>>>
array([ 3,  4,  5,  6,  7,  8,  9, 10])


x = 4
arr[arr>x]
>>>
array([ 5,  6,  7,  8,  9, 10])


NUMPY OPERATIONS:
-----------------

import numpy as np
arr = np.arange(0,10)

arr + arr
>>> array([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])

arr - arr
>>> array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

arr * arr 
>>> array([ 0,  1,  4,  9, 16, 25, 36, 49, 64, 81])

arr**3
>>> array([  0,   1,   8,  27,  64, 125, 216, 343, 512, 729])


#Taking Square Roots
np.sqrt(arr)
>>> array([ 0.,1.,1.41421356,1.73205081,2.,2.23606798,2.44948974,2.64575131,2.82842712,3.])

np.log(arr), np.max(arr) #same as arr.max(), 
np.sin(arr) and etc....


FEW NUMPY EXAMPLES -
-------------------

Create a 3x3 matrix with values ranging from 0 to 8?
np.arange(0,9).reshape(3,3)
>>>
array([[0, 1, 2],
       [3, 4, 5],
       [6, 7, 8]])
	   
	   

Get the sum of all the values in mat?
mat.sum()

Get the standard deviation of the values in mat?
mat.std()

Get the sum of all the columns in mat?
mat.sum(axis = 0)

-------------------------------------------------------------------------------------------------------------------------------------------------------

PANDAS LIBRARY:
---------------

SERIES:
------
A Series is very similar to a NumPy array (in fact it is built on top of the NumPy array object). WHAT DIFFERENTIATES the NumPy array from a Series, is that a Series can have axis labels, meaning it can be indexed by a label, instead of just a number location. It also doesn't need to hold numeric data, it can hold any arbitrary Python Object.


import numpy as np
import pandas as pd

labels = ['a','b','c']
my_list = [10,20,30]
arr = np.array([10,20,30])
d = {'a':10,'b':20,'c':30}



pd.Series(data = my_list)  #creating series using lists
>>>
0    10
1    20
2    30
dtype: int64


pd.Series(data=my_list,index=labels) #OR pd.Series(my_list,labels)
>>>
a    10
b    20
c    30
dtype: int64

#Similarly pd.Series(arr,labels) and pd.Series(d) can be used to get same output as above.


DATAFRAMES IN PANDAS:
---------------------
We can think of a DataFrame as a bunch of Series objects put together to share the same index.

import pandas as pd
import numpy as np
from numpy.random import randn

df = pd.DataFrame(randn(5,4),index='A B C D E'.split(),columns='W X Y Z'.split())
df

Selection and Indexing:
-----------------------
Let's learn the various methods to grab data from a DataFrame

df['W']
>>>
		W	
A    0.229845
B   -0.333362
C   -0.846333
D   -0.006989
E   -1.259322
Name: W, dtype: float64


# Pass a list of column names
df[['W','Z']]
>>>

	  W      	Z
A  0.229845   1.170436
B  -0.333362  -1.089344
C  -0.846333  0.363177 
D  -0.006989  1.566062
E  -1.259322  0.732093 

type(df['W'])
>>> pandas.core.series.Series

Creating a new column:
----------------------
df['new'] = df['W'] + df['Y']

REMOVING COLUMNS:
-----------------
df.drop('new',axis=1,inplace = True) #inplace to permanently remove the col

Can also drop rows this way:
df.drop('E',axis=0)   # Eth row removed

Selecting Rows:
df.loc['A']
>>>
W    2.706850
X    0.628133
Y    0.907969
Z    0.503826
Name: A, dtype: float64

Or select based off of position instead of label?
df.iloc[2]
>>>
W   -2.018168
X    0.740122
Y    0.528813
Z   -0.589001
Name: C, dtype: float64



CONDITIONAL SELECTION:
----------------------
#SELECTING ALL THE COLUMNS FOR ROWS WHERE 'W'> 0
df[df['W']>0]

#SELECTING THE Y AND X COLUMN VALUES FOR ALL THE ROWS WHERE 'W' > 0
df[df['W']>0][['Y','X']]

#For two conditions you can use | and & with parenthesis:
df[(df['W']>0) & (df['Y'] > 0)]


MISSING DATA HANDLING:
---------------------

import numpy as np
import pandas as pd

df = pd.DataFrame({'A':[1,2,np.nan],'B':[5,np.nan,np.nan],'C':[1,2,3]})
df
>>>
		A 	 B	  C
0		1.0 5.0   1
1		2.0 NaN   2 
2		NaN NaN   3

#Removes all null value rows from df
df.dropna()
>>>
		A 	 B	  C
0		1.0 5.0   1

df.dropna(axis=1)	#removes all columns having Nan values
>>>
   C
0  1 
1  2 
2  3

#fills all NaN values with 'FILL VALUE' STRING
df.fillna(value='FILL VALUE')
>>>

		A 	 		B	  				C
0		1.0 		5.0   				1
1		2.0 		FILL VALUE   		2 
2		FILL VALUE  FILL VALUE   		3


#Fills NaN with mean for Column 'A'
df['A'].fillna(value=df['A'].mean())


GROUP BY:
---------
df.groupby('Company').mean()
df.groupby('Person').std()
df.groupby('Sales').max()


MERGING, JOINING and CONCATENATING:
-----------------------------------

import pandas as pd

df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],
                    'B': ['B0', 'B1', 'B2', 'B3'],
                    'C': ['C0', 'C1', 'C2', 'C3'],
                    'D': ['D0', 'D1', 'D2', 'D3']},
                    index=[0, 1, 2, 3])
					

df2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'],
                    'B': ['B4', 'B5', 'B6', 'B7'],
                    'C': ['C4', 'C5', 'C6', 'C7'],
                    'D': ['D4', 'D5', 'D6', 'D7']},
                    index=[4, 5, 6, 7])
					

df3 = pd.DataFrame({'A': ['A8', 'A9', 'A10', 'A11'],
                    'B': ['B8', 'B9', 'B10', 'B11'],
                    'C': ['C8', 'C9', 'C10', 'C11'],
                    'D': ['D8', 'D9', 'D10', 'D11']},
                    index=[8, 9, 10, 11])


CONCATENATION -
---------------
**DIMENSIONS SHOULD MATCH FOR CONCATENATING.


pd.concat([df1,df2,df3])

>>>
	A	B	C	D
0	A0	B0
1	A1	B1
2	A2	B2
3	A3	B3
4	A4	B4
5	A5	B5	
6	A6	B6
7	A7	B7
8	A8	B8
9	A9	B9
10	A10	B10
11	A11	B11...


MERGE:
------

The merge function allows you to merge DataFrames together using a similar logic as merging SQL Tables together. For example:

left = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],
                     'A':   ['A0', 'A1', 'A2', 'A3'],
                     'B':   ['B0', 'B1', 'B2', 'B3']})
   
right = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3'],
                       'C':  ['C0', 'C1', 'C2', 'C3'],
                       'D':  ['D0', 'D1', 'D2', 'D3']})
					   
					   
pd.merge(left,right,how='inner',on='key')
>>>
	A  B  KEY C D
0	A0 B0 K0 C0 D0
1	A1 B1 K1 C1 D1
2	A2 B2 K2 C2 D2
3	A3 B3 K3 C3 D3 


MORE COMPLEX EXAMPLE??
----------------------
left = pd.DataFrame({'key1': ['K0', 'K0', 'K1', 'K2'],
                     'key2': ['K0', 'K1', 'K0', 'K1'],
                     'A':    ['A0', 'A1', 'A2', 'A3'],
                     'B':    ['B0', 'B1', 'B2', 'B3']})
    
right = pd.DataFrame({'key1': ['K0', 'K1', 'K1', 'K2'],
                      'key2': ['K0', 'K0', 'K0', 'K0'],
                      'C':    ['C0', 'C1', 'C2', 'C3'],
                      'D':    ['D0', 'D1', 'D2', 'D3']})
					  
					  
					  
pd.merge(left, right, on=['key1', 'key2'])
>>>
	key1 key2 A  B  C  D
0	K0   K0   A0 B0 C0 D0
1	K1   K0   A2 B2 C1 D1
2	K1   K0   A2 B2 C2 D2 


pd.merge(left, right, how='right', on=['key1', 'key2'])

pd.merge(left, right, how='left', on=['key1', 'key2'])



JOINING --
---------

left = pd.DataFrame({'A': ['A0', 'A1', 'A2'],
                     'B': ['B0', 'B1', 'B2']},
                      index=['K0', 'K1', 'K2']) 

right = pd.DataFrame({'C': ['C0', 'C2', 'C3'],
                    'D': ['D0', 'D2', 'D3']},
                      index=['K0', 'K2', 'K3'])
					  
					  
left.join(right)
>>>
	A	B  C  D 
K0	A0 B0 C0  D0 
K1	A1 B1 NaN NaN 
K2	A2 B2 C2  D2 

left.join(right, how='outer')
>>>
	A   B   C   D
K0	A0  B0  C0  D0 
K1	A1  B1  NaN NaN 
K2	A2  B2  C2  D2 
K3	NaN NaN C3  D3


PANDAS OPERATIONS:
------------------

import pandas as pd

df = pd.DataFrame({'col1':[1,2,3,4],'col2':[444,555,666,444],'col3':['abc','def','ghi','xyz']})
df.head()

>>>
  col1	col2	col3
0 1 	444 	abc 
1 2 	555 	def 
2 3 	666 	ghi 
3 4 	444	 	xyz


UNIQUE VALUES:
df['col2'].unique()
>>> array([444, 555, 666], dtype=int64)

df['col2'].nunique()
>> 3

df['col2'].value_counts()
>>> 
444    2
555    1
666    1
Name: col2, dtype: int64

SELECTING DATA:
---------------

#Select from DataFrame using criteria from multiple columns
newdf = df[(df['col1']>2) & (df['col2']==444)]
>>> 
  col1	col2	col3
3 4 	444	 	xyz


APPLYING FUNCTIONS:
-------------------

def times2(x):
	return x*2


df['col1'].apply(times2)
>>>
0    2
1    4
2    6
3    8
Name: col1, dtype: int64


df['col3'].apply(len)
>>>
0    3
1    3
2    3
3    3
Name: col3, dtype: int64


df['col1'].sum()  --> 10


Permanently Removing a Column?
-------------------------------
del df['col1']


Get column and index names?
df.columns --> Index(['col2', 'col3'], dtype='object')

df.index   --> RangeIndex(start=0, stop=4, step=1)


Sorting and Ordering a DataFrame?
---------------------------------

df.sort_values(by='col2') #inplace=False by default

Find Null Values or Check for Null Values?
--------------------------------------------
df.isnull()

Drop the rows with NaN values?
------------------------------
df.dropna()


Filling in NaN values with something else?
-------------------------------------------

df = pd.DataFrame({'col1':[1,2,3,np.nan],
                   'col2':[np.nan,555,666,444],
                   'col3':['abc','def','ghi','xyz']})
df.head()


>>>
   col1 col2   col3 
0	1.0 NaN    abc
1	2.0 555.0  def
2   3.0 666.0  ghi
3   NaN 444.0  xyz

df.fillna('FILL')

>>>
   col1 col2   col3 
0	1.0 FILL    abc
1	2.0 555.0  def
2   3.0 666.0  ghi
3   FILL 444.0  xyz
-----------------------------------------------------------------------------------------------------------------------------------------------

Python with Oracle Connection:
-------------------------------
import cx_Oracle

dsn_tns = cx_Oracle.makedsn('Host Name', 'Port Number', service_name='Service Name') # if needed, place an 'r' before any parameter in order to address special characters such as '\'.
conn = cx_Oracle.connect(user=r'User Name', password='Personal Password', dsn=dsn_tns) # if needed, place an 'r' before any parameter in order to address special characters such as '\'. For example, if your user name contains '\', you'll need to place 'r' before the user name: user=r'User Name'

c = conn.cursor()
c.execute('select * from database.table') # use triple quotes if you want to spread your query across multiple lines
for row in c:
    print (row[0], '-', row[1], '-', row[2]) # this only shows the first two columns. To add an additional column you'll need to add , '-', row[2], etc.

	
conn.close()


-----------------------------------------------------------------------------------------------------------------------------------------------


DATA INPUT AND OUTPUT:
----------------------

import numpy as np
import pandas as pd

CSV INPUT:
df = pd.read_csv('example')

CSV OUTPUT:
df.to_csv('example',index = False)

EXCEL INPUT:
pd.read_excel('C:/Users/Rahul_Tiwari4/Downloads/Py-DS-ML-Bootcamp-master/Refactored_Py_DS_ML_Bootcamp-master/03-Python-for-Data-Analysis-Pandas/Excel_Sample.xlsx',sheet_name='Sheet1')

EXCEL OUTPUT:
df.to_excel('Excel_Sample.xlsx',sheet_name='Sheet1')


HTML INPUT:
df = pd.read_html('http://www.fdic.gov/bank/individual/failed/banklist.html')


--------------------------------------------------------------------------------------------------------------------------------------------------------------
EXPLORATORY DATA ANALYSIS ----

MATPLOTLIB AND SEABORN -
-----------------------

#LINE CHARTS -
from matplotlib import pyplot as plt
years = [1950, 1960, 1970, 1980, 1990, 2000, 2010]
gdp = [300.2, 543.3, 1075.9, 2862.5, 5979.6, 10289.7, 14958.3]

# create a line chart, years on x-axis, gdp on y-axis
plt.plot(years, gdp, color='green', marker='o', linestyle='solid')

# add a title
plt.title("Nominal GDP")

# add a label to the y-axis
plt.xlabel('Year')
plt.ylabel("Billions of $")
plt.show()


EXAMPLE2)
#Multiple line charts on One Graph
variance = [1, 2, 4, 8, 16, 32, 64, 128, 256]
bias_squared = [256, 128, 64, 32, 16, 8, 4, 2, 1]
total_error = [x + y for x, y in zip(variance, bias_squared)]
xs = [i for i, _ in enumerate(variance)]

# we can make multiple calls to plt.plot
# to show multiple series on the same chart
plt.plot(xs, variance, 'g-', label='variance') # green solid line
plt.plot(xs, bias_squared, 'r-.', label='bias^2') # red dot-dashed line
plt.plot(xs, total_error, 'b:', label='total error') # blue dotted line

# because we've assigned labels to each series
# we can get a legend for free
# loc=9 means "top center"
plt.legend(loc=9)
plt.xlabel("model complexity")
plt.title("The Bias-Variance Tradeoff")
plt.show()

----------------------------------------------------------------------------------------


#BAR CHARTS --
A bar chart is a good choice when you want to show how some quantity varies among some discrete set of items. For instance, To show how many 
Academy Awards were won by each of a variety of movies?

movies = ["Annie Hall", "Ben-Hur", "Casablanca", "Gandhi", "West Side Story"]
num_oscars = [5, 11, 3, 8, 10]

# bars are by default width 0.8, so we'll add 0.1 to the left coordinates
# so that each bar is centered
xs = [i + 0.1 for i, _ in enumerate(movies)]


# plot bars with left x-coordinates [xs], heights [num_oscars],3RD ARGUMENT IS OPTIONAL FOR BARWIDTH**
plt.bar(xs, num_oscars)
plt.ylabel("# of Academy Awards")
plt.title("My Favorite Movies")


# label x-axis with movie names at bar centers
plt.xticks([i + 0.5 for i, _ in enumerate(movies)], movies)
plt.show()


------------------------------------------------------------------------

SCATTER PLOTS --
--------------

friends = [ 70, 65, 72, 63, 71, 64, 60, 64, 67]
minutes = [175, 170, 205, 120, 220, 130, 105, 145, 190]
labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i']
plt.scatter(friends, minutes)

# label each point
for label, friend_count, minute_count in zip(labels, friends, minutes):
	plt.annotate(label,
	xy=(friend_count, minute_count), # put the label with its point
	xytext=(5, -5), # but slightly offset
	textcoords='offset points')
	
plt.title("Daily Minutes vs. Number of Friends")
plt.xlabel("# of friends")
plt.ylabel("daily minutes spent on the site")
plt.show()


Example2)
test_1_grades = [ 99, 90, 85, 97, 80]
test_2_grades = [100, 85, 60, 90, 70]

plt.scatter(test_1_grades, test_2_grades)
plt.title("Axes Aren't Comparable")
plt.xlabel("test 1 grade")
plt.ylabel("test 2 grade")
plt.show()

-------------------------------------------------------------------------------------


HISTOGRAMS --
----------

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

x = np.random.random_integers(1, 100, 5)
plt.hist(x, bins=20)
plt.ylabel('No of times')
plt.show()


------------------------------------------------------------------------------------

PIE CHART --
----------

values = [12, 55, 4, 32, 14]
colors = ['r', 'g', 'b', 'c', 'm']
explode = [0, 0, 0.2, 0, 0]
labels = ['India', 'United States', 'Russia', 'China', 'Europe']

plt.pie(values, colors= colors, labels=labels, explode = explode)
plt.title('Student Locations')
plt.show()


BOXPLOTS --
----------

uniformSkewed = np.random.rand(100) * 100 - 40
high_outliers = np.random.rand(10) * 50 + 100
low_outliers = np.random.rand(10) * -50 - 100

data = np.concatenate((uniformSkewed, high_outliers, low_outliers))
plt.boxplot(data)
plt.show()


-------------------------------------------------------------------------------------

DATA PROCESSING - 
---------------

--"Converting to date format.."
        train_df["Date"] = (pd.to_datetime(train_df["Datetime"], format="%d-%m-%Y %H:%M"))
        test_df["Date"] = (pd.to_datetime(test_df["Datetime"], format="%d-%m-%Y %H:%M"))

--"Creating variables from date field.."
        train_df["Year"] = train_df["Date"].apply(lambda x: x.year)
        test_df["Year"] = test_df["Date"].apply(lambda x: x.year)

	train_df["Hour"] = train_df["Date"].apply(lambda x: x.hour)
        test_df["Hour"] = test_df["Date"].apply(lambda x: x.hour)

	train_df["WeekDay"] = train_df["Date"].apply(lambda x: x.weekday())
	test_df["WeekDay"] = test_df["Date"].apply(lambda x: x.weekday())

	train_df["DayCount"] = train_df["Date"].apply(lambda x: x.toordinal())
        test_df["DayCount"] = test_df["Date"].apply(lambda x: x.toordinal())



-------------------------------------------------------------------------------------------------------------------------------------------------------

Data Preprocessing Template:
----------------------------

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Data.csv')
X = dataset.iloc[:, :-1].values  #iloc means locate indexes - Dataset[rows range,columns range].values
y = dataset.iloc[:, -1].values
print(X)
print(y)

# Taking care of missing data
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(X[:, 1:3])						#this will actually find the missing values in column mentioned
X[:, 1:3] = imputer.transform(X[:, 1:3])	#this will handle the missing values as mentioned in the params for SimpleImputer Class.
print(X)


# Encoding categorical data
# Encoding the Independent Variable
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')
X = np.array(ct.fit_transform(X))
print(X)

# Encoding the Dependent Variable
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(y)
print(y)



# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)
print(X_train)
print(X_test)
print(y_train)
print(y_test)



# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])
X_test[:, 3:] = sc.transform(X_test[:, 3:])
print(X_train)
print(X_test)


Machine Learning Regression Algorithms :
---------------------------------------



Implementing Simple Linear Regression -->
-----------------------------------------
Simple Linear Regression:
-------------------------
In this process, a relationship is established between independent and dependent variables by fitting them to a line. This line is known as 
the regression line and represented by a linear equation:
					Y= aX + b.

In this equation:
Y – Dependent Variable
a – Slope
X – Independent variable
b – Intercept
The coefficients a & b are derived by minimizing the sum of the squared difference of distance between data points and the regression line.

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Salary_Data.csv')
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, 1].values

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1/3, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)
sc_y = StandardScaler()
y_train = sc_y.fit_transform(y_train.reshape(-1,1))

# Fitting Simple Linear Regression to the Training set
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train)

# Predicting the Test set results
y_pred = regressor.predict(X_test)

# Visualising the Training set results
plt.scatter(X_train, y_train, color = 'red')
plt.plot(X_train, regressor.predict(X_train), color = 'blue')
plt.title('Salary vs Experience (Training set)')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show()

# Visualising the Test set results
plt.scatter(X_test, y_test, color = 'red')
plt.plot(X_train, regressor.predict(X_train), color = 'blue')
plt.title('Salary vs Experience (Test set)')
plt.xlabel('Years of Experience')
plt.ylabel('Salary')
plt.show()




Multiple Linear Regression -->
-------------------------------
When the features are multiple and not single then this Regression is called Multiple Linear Regression.
Equation becomes - 
			Y = aX1 +bX2+ cX3+ d
			
Here in the above equation x1,x2,x3 are different features and d is the intercept.			

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('50_Startups.csv')
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, 4].values

# Encoding categorical data
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder = LabelEncoder()
X[:, 3] = labelencoder.fit_transform(X[:, 3])
onehotencoder = OneHotEncoder(categorical_features = [3])
X = onehotencoder.fit_transform(X).toarray()

# Avoiding the Dummy Variable Trap
X = X[:, 1:]

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)
sc_y = StandardScaler()
y_train = sc_y.fit_transform(y_train.reshape(-1,1))

# Fitting Multiple Linear Regression to the Training set
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train)

# Predicting the Test set results
y_pred = regressor.predict(X_test)



Polynomial Regression -->
--------------------------
When the equation of the Regression has higher degrees/powers of single Column or feature then that model is known as Polynomial Regression.

Y = ax + bx^2 + cx^3 + dx^4

Here in the above equation same column has multiple degrees.

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Position_Salaries.csv')
X = dataset.iloc[:, 1:2].values
y = dataset.iloc[:, 2].values

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)

# Fitting Linear Regression to the dataset
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X, y)

# Fitting Polynomial Regression to the dataset
from sklearn.preprocessing import PolynomialFeatures
poly_reg = PolynomialFeatures(degree = 4)
X_poly = poly_reg.fit_transform(X)
poly_reg.fit(X_poly, y)
lin_reg_2 = LinearRegression()
lin_reg_2.fit(X_poly, y)

# Visualising the Linear Regression results
plt.scatter(X, y, color = 'red')
plt.plot(X, lin_reg.predict(X), color = 'blue')
plt.title('Truth or Bluff (Linear Regression)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()

# Visualising the Polynomial Regression results
plt.scatter(X, y, color = 'red')
plt.plot(X, lin_reg_2.predict(poly_reg.fit_transform(X)), color = 'blue')
plt.title('Truth or Bluff (Polynomial Regression)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()

# Visualising the Polynomial Regression results (for higher resolution and smoother curve)
X_grid = np.arange(min(X), max(X), 0.1)
X_grid = X_grid.reshape((len(X_grid), 1))
plt.scatter(X, y, color = 'red')
plt.plot(X_grid, lin_reg_2.predict(poly_reg.fit_transform(X_grid)), color = 'blue')
plt.title('Truth or Bluff (Polynomial Regression)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()

# Predicting a new result with Linear Regression
lin_reg.predict([[6.5]])

# Predicting a new result with Polynomial Regression
lin_reg_2.predict(poly_reg.fit_transform([[6.5]]))



SVR --> Support Vector Regression
----------------------------------
Support Vector Regression (SVR) uses the same principle as SVM(Support Vector Machines for Classification tasks).
Main aim for SVR is to decide a decision boundary at ‘a’ distance from the original hyperplane such that data points closest to the hyperplane or
the support vectors are within that boundary line. Hence, we are going to take only those points that are within the decision boundary and have 
the least error rate, or are within the Margin of Tolerance. This gives us a better fitting model.

Assuming that the equation of the hyperplane is as follows:
Y = wx+b (equation of hyperplane)

Then the equations of decision boundary become:
wx+b= +a and wx+b= -a

Thus, any hyperplane that satisfies our SVR should satisfy below condition:
-a < Y- wx+b < +a 


# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Position_Salaries.csv')
X = dataset.iloc[:, 1:2].values
y = dataset.iloc[:, 2].values

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
sc_y = StandardScaler()
X = sc_X.fit_transform(X)
y = sc_y.fit_transform(y.reshape(-1,1))

# Fitting SVR to the dataset
from sklearn.svm import SVR
regressor = SVR(kernel = 'rbf')
regressor.fit(X, y)

# Predicting a new result
y_pred = regressor.predict([[6.5]])
y_pred = sc_y.inverse_transform(y_pred)

# Visualising the SVR results
plt.scatter(X, y, color = 'red')
plt.plot(X, regressor.predict(X), color = 'blue')
plt.title('Truth or Bluff (SVR)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()

# Visualising the SVR results (for higher resolution and smoother curve)
X_grid = np.arange(min(X), max(X), 0.01) # choice of 0.01 instead of 0.1 step because the data is feature scaled
X_grid = X_grid.reshape((len(X_grid), 1))
plt.scatter(X, y, color = 'red')
plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')
plt.title('Truth or Bluff (SVR)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()



Decision Tree Regression -->
----------------------------
A decision tree is a flowchart-like structure in which each internal node represents a test on a feature (e.g. whether a coin flip comes up heads
 or tails) , each leaf node represents a class label (decision taken after computing all features) and branches represent conjunctions of features 
 that lead to those class labels. The paths from root to leaf represent classification rules.

Tree models where the target variable can take a discrete set of values are called CLASSIFICATION trees. 
DECISION TREES where the target variable can take CONTINUOUS values (typically real numbers) are called REGRESSION trees. 
Classification And Regression Tree (CART) is general term for this.

While making decision tree, at each node of tree we ask different type of questions. Based on the asked question we will calculate the information 
gain corresponding to it. Algorithm for constructing decision tree usually works top-down, by choosing a variable at each step that best splits the 
set of items. Different algorithms use different metrices for measuring best.

Information gain->> 
is used to decide which feature to split on at each step in building the tree. Simplicity is best, so we want to keep our 
tree small. To do so, at each step we should choose the split that results in the purest daughter nodes. A commonly used measure of purity is 
called information. For each node of the tree, the information value measures how much information a feature gives us about the class. The 
split with the highest information gain will be taken as the first split and the process will continue until all children nodes are pure, or 
until the information gain is 0.

Advantage of Decision Tree:
---------------------------
Easy to use and understand.
Can handle both categorical and numerical data.
Resistant to outliers, hence require little data preprocessing.

Disadvantage of Decision Tree
-----------------------------
Prone to overfitting.
Require some kind of measurement as to how well they are doing.
Need to be careful with parameter tuning.
Can create biased learned trees if some classes dominate.


# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Position_Salaries.csv')
X = dataset.iloc[:, 1:2].values
y = dataset.iloc[:, 2].values

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)
sc_y = StandardScaler()
y_train = sc_y.fit_transform(y_train.reshape(-1,1))

# Fitting Decision Tree Regression to the dataset
from sklearn.tree import DecisionTreeRegressor
regressor = DecisionTreeRegressor(random_state = 0)
regressor.fit(X, y)

# Predicting a new result
y_pred = regressor.predict([[6.5]])

# Visualising the Decision Tree Regression results (higher resolution)
X_grid = np.arange(min(X), max(X), 0.01)
X_grid = X_grid.reshape((len(X_grid), 1))
plt.scatter(X, y, color = 'red')
plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')
plt.title('Truth or Bluff (Decision Tree Regression)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()


Random Forest Regression -->
-----------------------------
Every decision tree has high variance, but when we combine all of them together in parallel then the resultant variance is low as each decision
 tree gets perfectly trained on that particular sample data and hence the output doesn’t depend on one decision tree but multiple decision trees. 
 In the case of a classification problem, the final output is taken by using the majority voting classifier. In the case of a regression problem, 
 the final output is the mean of all the outputs. This part is Aggregation.
 
A Random Forest is an ensemble technique capable of performing both regression and classification tasks with the use of multiple decision trees and 
a technique called Bootstrap and Aggregation, commonly known as BAGGING. The basic idea behind this is to combine multiple decision trees in 
determining the final output rather than relying on individual decision trees.
Random Forest has multiple decision trees as base learning models. We randomly perform row sampling and feature sampling from the dataset forming 
sample datasets for every model. This part is called BOOTSTRAP.



# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Position_Salaries.csv')
X = dataset.iloc[:, 1:2].values
y = dataset.iloc[:, 2].values

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)
sc_y = StandardScaler()
y_train = sc_y.fit_transform(y_train.reshape(-1,1))

# Fitting Random Forest Regression to the dataset
from sklearn.ensemble import RandomForestRegressor
regressor = RandomForestRegressor(n_estimators = 10, random_state = 0)
regressor.fit(X, y)

# Predicting a new result
y_pred = regressor.predict([[6.5]])

# Visualising the Random Forest Regression results (higher resolution)
X_grid = np.arange(min(X), max(X), 0.01)
X_grid = X_grid.reshape((len(X_grid), 1))
plt.scatter(X, y, color = 'red')
plt.plot(X_grid, regressor.predict(X_grid), color = 'blue')
plt.title('Truth or Bluff (Random Forest Regression)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()


============================================================================================================


CLASSIFICATION ALGORITHMS -->
------------------------------

Logistic Regression
--------------------
It is used to estimate discrete/Categorical values(usually binary values like 0/1) from a set of independent variables. It helps predict the 
probability of an event by fitting data to a logit/Sigmoid function. It is also called logit regression.
Most Common type of Classification task is on Binomial Data where there are two types of classes 1/0 etc.
Multi-nomial Data Set has columns which have 3 or more categories.
Ordinal Data Set has columns which have values like examples Ratings Column(1 to 5). Or Reviews Column(Good,Moderate,Bad etc..)


# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Social_Network_Ads.csv')
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Fitting Logistic Regression to the Training set
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state = 0)
classifier.fit(X_train, y_train)

# Predicting the Test set results
y_pred = classifier.predict(X_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

# Visualising the Training set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Logistic Regression (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()



# Visualising the Test set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Logistic Regression (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()



K-Nearest Neighbors (K-NN)
--------------------------
KNNs usually calculates the Euclidean(Mostly Used) or Manhattan Distances to classify the point among different sets of points.
Generally n_neighbors = 5 is used that means 5 nearest points would be used around the point which will help us to decide and classify the outcome.


This algorithm can be applied to both classification and regression problems. Apparently, within the Data Science industry, it's more widely used 
to solve classification problems. It’s a simple algorithm that stores all available cases and classifies any new cases by taking a majority vote 
of its k neighbors. The case is then assigned to the class with which it has the most in common. A distance function performs this measurement.
KNN can be easily understood by comparing it to real life. For example, if you want information about a person, it makes sense to talk to his or 
her friends and colleagues.

Things to consider before selecting KNN: 
KNN is computationally expensive
Variables should be normalized, or else higher range variables can bias the algorithm
Data still needs to be pre-processed.


# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Social_Network_Ads.csv')
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Fitting K-NN to the Training set
from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifier.fit(X_train, y_train)

# Predicting the Test set results
y_pred = classifier.predict(X_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

# Visualising the Training set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('K-NN (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

# Visualising the Test set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('K-NN (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()



Support Vector Machine (SVM)
----------------------------
It is a classification method. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) 
with the value of each feature being the value of a particular coordinate.

For example, if we only had two features like Height and Hair length of an individual, we’d first plot these two variables in two dimensional space 
where each point has two co-ordinates (these co-ordinates are known as Support Vectors).
Now, we will find some line that splits the data between the two differently classified groups of data. This will be the line such that the 
distances from the closest point in each of the two groups will be farthest away.

In the example shown above, the line which splits the data into two differently classified groups is the black line, since the two closest points 
are the farthest apart from the line. This line is our classifier. Then, depending on where the testing data lands on either side of the line, 
that’s what class we can classify the new data as.

Source: https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Social_Network_Ads.csv')
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Fitting SVM to the Training set
from sklearn.svm import SVC
classifier = SVC(kernel = 'linear', random_state = 0)
classifier.fit(X_train, y_train)

# Predicting the Test set results
y_pred = classifier.predict(X_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

# Visualising the Training set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('SVM (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

# Visualising the Test set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('SVM (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()



Kernel SVM
-----------

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Social_Network_Ads.csv')
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Fitting Kernel SVM to the Training set
from sklearn.svm import SVC
classifier = SVC(kernel = 'rbf', random_state = 0)
classifier.fit(X_train, y_train)

# Predicting the Test set results
y_pred = classifier.predict(X_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

# Visualising the Training set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Kernel SVM (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

# Visualising the Test set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Kernel SVM (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()



Naive Bayes(Based on Baye's theorem)
-------------------------------------
A Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.
Even if these features are related to each other, a Naive Bayes classifier would consider all of these properties independently when calculating 
the probability of a particular outcome.

A Naive Bayesian model is easy to build and useful for massive datasets. It's simple and is known to outperform even highly sophisticated 
classification methods.Naive Bayes uses a similar method to predict the probability of different class based on various attributes. This algorithm 
is mostly used in text classification and with problems having multiple classes.

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Social_Network_Ads.csv')
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Fitting Naive Bayes to the Training set
from sklearn.naive_bayes import GaussianNB
classifier = GaussianNB()
classifier.fit(X_train, y_train)

# Predicting the Test set results
y_pred = classifier.predict(X_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

# Visualising the Training set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Naive Bayes (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

# Visualising the Test set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Naive Bayes (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()



Decision Tree Classification
-----------------------------

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Social_Network_Ads.csv')
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Fitting Decision Tree Classification to the Training set
from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifier.fit(X_train, y_train)

# Predicting the Test set results
y_pred = classifier.predict(X_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

# Visualising the Training set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Decision Tree Classification (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

# Visualising the Test set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Decision Tree Classification (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()




Random Forest Classification
-----------------------------

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Social_Network_Ads.csv')
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Fitting Random Forest Classification to the Training set
from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
classifier.fit(X_train, y_train)

# Predicting the Test set results
y_pred = classifier.predict(X_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

# Visualising the Training set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Random Forest Classification (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

# Visualising the Test set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Random Forest Classification (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()


==================================================================================================================


CLUSTERING ALGORITHMS -->
---------------------------

K-Means Clustering
------------------
It is an unsupervised algorithm that solves clustering problems. Data sets are classified into a particular number of clusters (let's call that 
number K) in such a way that all the data points within a cluster are homogenous and heterogeneous from the data in other clusters.

How K-means forms clusters:
The K-means algorithm picks 'k' number of points, called centroids, for each cluster.
Each data point forms a cluster with the closest centroids, i.e., K clusters.
It now creates new centroids based on the existing cluster members.
With these new centroids, the closest distance for each data point is determined. This process is repeated until the centroids do not change.


# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Mall_Customers.csv')
X = dataset.iloc[:, [3, 4]].values
# y = dataset.iloc[:, 3].values

# Splitting the dataset into the Training set and Test set
"""from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"""

# Feature Scaling
"""from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)
sc_y = StandardScaler()
y_train = sc_y.fit_transform(y_train.reshape(-1,1))"""

# Using the elbow method to find the optimal number of clusters
from sklearn.cluster import KMeans
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)
plt.plot(range(1, 11), wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

# Fitting K-Means to the dataset
kmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = 42)
y_kmeans = kmeans.fit_predict(X)

# Visualising the clusters
plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1')
plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')
plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')
plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')
plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids')
plt.title('Clusters of customers')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show()




Hierarchical Clustering :
--------------------------
Also called Hierarchical cluster analysis or HCA is an unsupervised clustering algorithm which involves creating clusters that have predominant 
ordering from top to bottom.
For e.g: All files and folders on our hard disk are organized in a hierarchy.
The algorithm groups similar objects into groups called clusters. The endpoint is a set of clusters or groups, where each cluster is distinct from 
each other cluster, and the objects within each cluster are broadly similar to each other.

This clustering technique is divided into two types:
Agglomerative Hierarchical Clustering
It's a “bottom-up” approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.

Divisive Hierarchical Clustering
In Divisive or DIANA(DIvisive ANAlysis Clustering) is a top-down clustering method where we assign all of the observations to a single cluster and
 then partition the cluster to two least similar clusters. Finally, we proceed recursively on each cluster until there is one cluster for each 
 observation. So this clustering approach is exactly opposite to Agglomerative clustering.

Source: https://www.kdnuggets.com/2019/09/hierarchical-clustering.html

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Mall_Customers.csv')
X = dataset.iloc[:, [3, 4]].values
# y = dataset.iloc[:, 3].values

# Splitting the dataset into the Training set and Test set
"""from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"""

# Feature Scaling
"""from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)
sc_y = StandardScaler()
y_train = sc_y.fit_transform(y_train.reshape(-1,1))"""

# Using the dendrogram to find the optimal number of clusters
import scipy.cluster.hierarchy as sch
dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))
plt.title('Dendrogram')
plt.xlabel('Customers')
plt.ylabel('Euclidean distances')
plt.show()

# Fitting Hierarchical Clustering to the dataset
from sklearn.cluster import AgglomerativeClustering
hc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')
y_hc = hc.fit_predict(X)

# Visualising the clusters
plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'Cluster 1')
plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')
plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'Cluster 3')
plt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')
plt.scatter(X[y_hc == 4, 0], X[y_hc == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')
plt.title('Clusters of customers')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.legend()
plt.show()



================================================================================================================


ASSOCIATION RULE LEARNING -->
------------------------------

Apriori
--------

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Data Preprocessing
dataset = pd.read_csv('Market_Basket_Optimisation.csv', header = None)
transactions = []
for i in range(0, 7501):
    transactions.append([str(dataset.values[i,j]) for j in range(0, 20)])

# Training Apriori on the dataset
from apyori import apriori
rules = apriori(transactions, min_support = 0.003, min_confidence = 0.2, min_lift = 3, min_length = 2)

# Visualising the results
results = list(rules)



REINFORCEMENT LEARNING -->
---------------------------



======================================================================================================================================

INTERVIEW PREP TERMS?
----------------------

#L1,L2 REGRESSION(LASSO,RIDGE).
#HYPERPARAMETER TUNING.
#EVALUATION OF MODELS.
#OLS,RMSE,MAE, PRECISION,RECALL,F1_SCORE,ACCURACY SCORES,ROC,AUC CURVES,CONFUSION MATRIX.
#BIAS,VARIANCE TRADE-OFF, UNDERFITTING,OVERFITTING.
#MULTI-COLLINEARITY,
#HOW TO RESOLVE IMBALANCED DATA-SET ISSUE?
#OUTLIER DETECTION/HANDLING.
#Model Search RANDOMIZED SEARCH,GRID SEARCH etc..
#CROSS_VALIDATION techniques etc..
#HYPOTHESIS, A/B TESTING.
#P-values concept. T-test,Chi-square test, ANOVA Test etc...
#HOMOSCEDASTICITY, HETEROSCEDASTICITY.
DIMENSIONALITY REDUCTION TECHNIQUES ..PCA ,LDA, Kernel PCA etc.
ENSEMBLE TREES - Bagging and Boosting Techniques XGBoost, AdaBoost etc..
Feature Selection and Scaling.
GINI COEFFICIENT.
ENTROPY IN DECISION TREE,INFORMATION GAIN.
Clustering Techniques - Hierarchical Clustering.
Gradient Boosting etc..Gradient-Descent. 
Model Deployment in AWS, GCP etc..


NLP CONCEPTS -
--------------
TEXT CLASSIFICATION, STEMMING, LEMMATIZATION,TOKENIZATION etc..
Bag of Words, TF-IDF, SPAM CLASSIFIER etc.
Word2Vec, FAKE NEWS CLASSIFIER.
WORD EMBEDDING






Q1) RIDGE and LASSO Regression - REGULARIZATION Techniques?
Ans1) 
Since Linear regression works by selecting coefficients for each independent variable that minimizes a loss function. 
However, if the coefficients are too large, it can lead to model over-fitting on the training dataset. 
Such a model will not generalize well on the unseen data. To overcome this shortcoming, we do regularization which penalizes large 
coefficients.
Ridge regression and Lasso regression are two popular techniques that make use of regularization for predicting.
Both the techniques work by penalising the magnitude of coefficients of features along with minimizing the error between predictions and actual 
values or records. The key difference however, between Ridge and Lasso regression is that Lasso Regression has the ability to nullify the impact 
of an irrelevant feature in the data, meaning that it can reduce the coefficient of a feature to zero thus completely eliminating it and hence is 
better at reducing the variance when the data consists of many insignificant features. Ridge regression, however, can not reduce the coefficients
to absolute zero. Ridge regression performs better when the data consists of features which are sure to be more relevant and useful.

What is Regularization?
Overfitting is one of the most annoying things about a Machine Learning model. 
After all those time-consuming processes that took to gather the data, clean and preprocess it, 
the model is still incapable to give out an optimised result.  There can be lots of noises in data 
which may be the variance in the target variable for the same and exact predictors or irrelevant features or 
it can be corrupted data points. The ML model is unable to identify the noises and hence uses them as well to train the model. 
This can have a negative impact on the predictions of the model. This is called OVERFITTING.

Ridge(L2) Regression -
-----------------
Ridge regression is an extension of linear regression where the loss function is modified to minimize the complexity of the model. 
This modification is done by adding a penalty parameter that is equivalent to the square of the magnitude of the coefficients.

LOSS FUNCTION = OLS + lambda * (Slope)^2

In the above loss function, lambda is the parameter we need to select. A low lambda value can lead to over-fitting, whereas a high lambda value can
lead to under-fitting.

Lasso(L1) Regression
-----------------
LEAST ABSOLUTE SHRINKAGE and SELECTION OPERATOR, is also a modification of linear regression. 
In Lasso, the loss function is modified to minimize the complexity of the model by limiting the sum of the absolute values of the model 
coefficients (also called the L1-norm).

The loss function for Lasso Regression can be expressed as below:

LOSS FUNCTION = OLS + lambda * Abs(|Slope|)

In the above loss function, lambda is the penalty parameter we need to select. Using an L1 norm constraint forces some weight values to zero 
to allow other coefficients to take non-zero values.


Summary:
--------
Ridge regression is very similar to Lasso regression just the difference is while penalizing the coefficients Ridge regression squares the 
differences i.e. Slope while for Lasso it penalizes by taking absolute value for the Slope.

Lasso Regression can exclude useless variables from equations making the equation simpler for a better Low Variance in Predictions.

Note --> CROSS-VALIDATION can be tried along with feature selection techniques to select most appropriate value of Lambda. 


ELASTICNET Regression -
-----------------------
ElasticNet combines the properties of both Ridge and Lasso regression. It works by penalizing the model using both the L2-norm and the L1-norm.


Below are the modules/libraries to import relevant Regressions from Scikit-learn.

from sklearn.linear_model import Ridge
reg = Ridge(alpha = 0.01)
reg.fit(X_train,y_train)

from sklearn.linear_model import Lasso
reg = Lasso(alpha = 0.01)
reg.fit(X_train,y_train)

from sklearn.linear_model import ElasticNet
reg = ElasticNet(alpha = 0.01)
reg.fit(X_train,y_train)

---------------------------------------------------------------------------------------------------------------------------------------------------


BIAS,VARIANCE Concepts and Trade-Off:
-------------------------------------

Bias refers to Training Error, the error which occurs while ML Model is still learning.
It is actually the error for our Training Data set. If this error is high i.e. HIGH Bias leads to UNDERFITTING Condition.

DEFINITION OF BIAS -> It is the difference between the avg prediction of our Model and the correct value which we are trying to predict.
Model with high bias pays very little attention to the training data and oversimplifies the model and thus leads to high error on both training and
test data.

Variance refers to the Testing Error, the error which occurs when our already trained ML model starts making huge errors in predicting the target
feature. This error is for Testing Data set mainly. Thus High variance leads to OVERFITTING condition.

DEFINITION OF VARIANCE ->
It is the variability of model prediction for a given data point or a value which tells about the spread of our data. Model with High Variance 
pays a lot of attention to training data and does not generalize on the data which it hasn't seen before. As a result this type of model
performs good with training data but high error rates with testing data.

Note: IDEALLY a ML Model should have both Low Bias and Low Variance.
If the model has High Bias then we can try bigger networks, train model for longer period of time or try different neural network Architectures.
If the model has High Variance then to reduce this we can get more Data, Use Regularization or try different neural network Architectures.
Regularization techniques are used for reducing the Variance.


BIAS-VARIANCE Trade-Off ->
If our model is simple and data has lesser no. of parameters then there could be possibility of High Bias and Low variance model and 
vice-versa i.e. if our model is complex and has large no. of features then it could result in HIGH Variance and Low Bias hence we need to have a 
model which could be a Balanced on without Overfitting and Underfitting condition.
This trade-off in model complexity is for the reason of Bias-Variance Trade Off and hence to build a good model we need to find a good balance
between Bias and Variance such that it minimalizes the Total Error.

		Total Error = Bias^2 + Variance + Irreducible Error(Noise)


An optimal Balance of Bias and Variance would never lead to Overfit or Underfit of a model.

--------------------------------------------------------------------------------------------------------------------------------------------------


CROSS-VALIDATION Technique:
---------------------------		

What is Cross-Validation?
Cross-Validation is basically a resampling technique to make our model sure about its efficiency and accuracy on the unseen data. 
In short, Model Validation technique, up for other applications.
Bunch of train/test splits —> testing accuracy for each split —> Average them.

Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample from the available data set.

K-Fold Cross Validation:
-----------------------
Quick Steps for Cross-Validation are:
1) Divide data into K partitions. These partitions will be of equal size.
2) Treat Fold-1 as test fold while K-1 as train folds.
3) Compute score of test-fold.
4) Repeat step 3 for all folds taking another fold as test while remaining as train.
5) Take Average of scores of all the folds.

It partitions our data into train/test in such a way that the previous set won’t repeat and for each set training and testing is performed once. 
Each partition (or fold) is of equal size. Can say, every datapoint plays as train data and test data in its life journey. This is basic 
functionality of cross-validation which can be useful while tuning hyper-parameters or selecting ML model such as logistic or decision trees for
classification problem. It prevents over-fitting and under-fitting by choosing optimal value of K. 
Also, an accurate model estimate than train_test_split method.


Types of Cross- Validation:
---------------------------

Leave One Out Cross Validation (LOOCV) :
----------------------------------------
This is very old technique which is replaced by k-fold and stratified k-fold but still useful in certain scenarios. Data is partitioned into 
blocks representing each with 1 record as test while remaining as train. Each and every record is treated as test and those many iterations to 
evaluate on each.

Example.
Data of 20 records means 20 partitions, each having one record. This leads to 20 iterations of training and evaluating.
In 1st iteration, model is tested on 1st block (fold) and tested on remaining 19 folds, giving out an accuracy. In the next iteration, 
another block having next record as test set while remaining as train, gives another accuracy. LOOCV makes testing and training on all 20 records 
in 20 iterations, hence expensive computational power. Average of all sets are then computed and thrown as a final accuracy aka k-fold score.

MERITS:
-> It gives sort of certainty of model performance when tested on unseen data.

LIMITATIONS:
-> As compared to train_test_split, it takes more computational power/time consuming .
-> As everything is tested and trained, leads to higher variance and testing unseen data in production leads to poor results.
Results can lead to higher variation as when an outlier datapoint is tested.


Leave-P-Out cross validation (LPOCV):
-------------------------------------
When using this exhaustive method, we take p number of points out from the total number of data points in the dataset(say n). While training the
 model we train it on these (n – p) data points and test the model on p data points. We repeat this process for all the possible combinations of p 
 from the original dataset. Then to get the final accuracy, we average the accuracies from all these iterations.

This is an exhaustive method as we train the model on every possible combination of data points. Remember if we choose a higher value for p, then 
the number of combinations will be more and we can say the method gets a lot more detailed,extensive.


K-Fold Cross-Validation:
------------------------
A variant of cross-validation where data is divided into partitions as train/test based on “K”. Here, K refers to any integer while fold is to a 
partition (or iteration). Model performs training on K-1 partitions and testing on Kth partition of data.

Example for 4-fold cross validation,
Data of 20 records, given 4-fold. Data is divided into 4 partitions. Each partition has (20/4=)5 records.
In 1st iteration, model tested on 1st block and trained on remaining 4 blocks, resulted an accuracy. In next iteration, another block as test set 
while remaining as train, resulting into another accuracy. This process of dividing and evaluating is done for all 5 folds. Average of all sets are 
computed and thrown as a final accuracy which is treated as a model accuracy.

Merits:
-> Overcome the problem of computational power like in LOO to some extent.
-> Since not every record is treated as test set like LOOCV hence, hence model may not be affected much if any outlier is present in data. 
   Overcomes problem of variability.

Limitations:
-> In any iteration, there is a possibility that test set may have records of just one class. This will make data imbalance and impact our model.



Stratified K-Fold Cross-Validation:
-----------------------------------
This is an improved version of K-Fold where now each fold has same percent of samples of each target class. Let’s say binary classification having 
dependent classes 1/0. Things will go wrong when only records of class 1 fall into test set and model is trained for class 0 and evaluated, results 
into data imbalance situation. Thus, stratified comes into picture.

Stratification is the process of rearranging the data so as to ensure that each fold is a good representative of the whole. 

For example, in a binary classification problem where each class comprises of 50% of the data, it is best to arrange the data such that in every 
fold, each class comprises of about half of the instances.


Rolling Cross Validation:
-------------------------
For time-series data the above-mentioned methods are not the best ways to evaluate the models. 
Here are two reasons as to why this is not an ideal way to go:

1) Shuffling the data messes up the time section of the data as it will disrupt the order of events
2) Using cross-validation, there is a chance that we train the model on future data and test on past data which will break the golden rule in time 
series i.e. “peaking in the future is not allowed”. Keeping these points in mind we perform cross validation in this manner

We create the fold (or subsets) in a forward-chaining fashion.
Suppose we have a time series for stock prices for a period of n years and we divide the data yearly into n number of folds. The folds would be 
created like:
iteration 1: training [1], test [2]
iteration 2: training [1 2], test [3]
iteration 3: training [1 2 3], test [4]
iteration 4: training [1 2 3 4], test [5]
iteration 5: training [1 2 3 4 5], test [6]
.
.
.
iteration n: training [1 2 3 ... n-1], test [n]


Here we can see in the above example that 1st iteration we train the data on the first year and then test it on 2nd Year.
and then in 2nd iteration we train the model on 1st,2nd year and test it on 3rd year data set and similarly it continues for subsequent numberof of
iterations.

Finally we can implement Cross-Validation Scikit-learn library cross_val_score.

>>> from sklearn import cross_validation
>>> df = cross_validation.KFold(len(train_set,n_folds = 10,indices = False)


There are many more libraries to implement various cross_validation techniques.

---------------------------------------------------------------------------------------------------------------------------------------------------


EVALUATION OF MODELS:
---------------------
Model Evaluation Metrics are required to quantify a Model Performance.
It depends on which ML tasks we are doing the evaluation metrics varies for different ML tasks like Regression/Classification/Clustering/Ranking etc..
Some metrics can be used for multiple tasks like precision-recall.

Metrics used for Classification Tasks:
---------------------------------------
Below are the Classification metrics through which we evaluate our Classification Model -
1) Classification Accuracy
2) Confusion matrix
3) Logarithmic Loss
4) Area under curve (AUC-ROC)
5) F1-Measure

Whenever evaluating model for Classification Problems mostly below outcomes occur.
1) True Positives(TP) - means the observation made belongs to a class and that observation actually is of that class.

2) True Negative(TN) -  means the observation made does not belongs to a class and that observation actually is not of that class.

3) False Positive(FP) - means the observation made belongs to a class and that observation actually is not of that class.

4) False Negative(FN) - means the observation made does not belongs to a class and that observation actually is of that class.


1) CLASSIFICATION ACCURACY - 
It is the ratio of number of correct predictions to the total number of predictions made.

Accuracy = Correct Predictions / Total Predictions Made.

How to implement?
-----------------
>>> from sklearn.metrics import accuracy_score
>>> print(accuracy_score(y_test,y_pred))


2) CONFUSION MATRIX -
As it name describes it gives in output a overall Matrix implementation of above all 4 outcomes(TP,TN,FP,FN).
Accuracy for Confusion Matrix can be calculated by below Formula - 

Accuracy = (TP + TN)/(TP + TN + FP + FN)


>>> from sklearn.metrics import confusion_matrix
>>> print(confusion_matrix(y_test,y_pred))


3) LOGARITHMIC LOSS - 
Logarithmic loss or log loss measures the performance of a classification model where the prediction input have a probability value between
0 and 1. It works well for multi-class classification.
 Log loss nearer to 0 indicates higher accuracy whereas away from 0 leads to lesser Accuracy.
 
 Hence minimising LOG LOSS leads up to Higher Accuracy.
 
 >>> from sklearn.metrics import log_loss
 >>> print(log_loss(y_test,y_pred))
 

4) ROC_AUC_SCORES/CURVES -
Area under the curve(AUC) for Receiver Operating Charateristic(ROC). Mainly this works for Binary Classification tasks but there are ways to extend
it to multi-class classification problems as well. It is actually for measuring ability of a model to discriminate between Positive and Negative
Classes. Better the AUC score better is the performance of our model. Below terms are important for understanding ROC AUC Scores and curves.

	TRUE POSITIVE RATE(TPR) or SENSITIVITY or RECALL:
			RECALL or Sensitivity or TPR =  TP / (TP + FN)
			
	TRUE NEGATIVE RATE(TNR) or SPECIFICITY:
			Specificity or TNR = TN / (TN + FP)
	
	PRECISION SCORE:
			Precision = TP / (TP + FP)

For better understanding refer - Blogs of Analytics Vidya, towardsdatascience, Statquest with Josh Stammer etc..


5) F1-Score or F-Measure:
It considers both precision and recall scores for computing the F1- Score for Classification tasks.
Sklearn metrics library has modules to compute all the above scores like precision,recall and f1-score as well.




Metrics used for Regression Tasks:
-----------------------------------
Most common metrics for Regression tasks are MAE(Mean Absolute Error), RMSE(Root Mean Squared Error).

1) Mean Absolute Error (MAE)- 
The sum of the absolute differences between predictions and actual values.

2) Root Mean Squared Error(RMSE) - 
It firstly computes the dfference between the prediction and actual values ,then does the square root of the difference and finally 
computes the square root to reach to the RMSE value.

>>> from sklearn.metrics import mean_squared_error,mean_absolute_error

Using above code we can easily calculate these two values.


3) R^2 Metric (Coeff. of Determination)-
It provides an indication of goodness of fit of a set of predictions made to the actual values.
R2 Score value varies from 0 to 1 where scores near to 0 means Badly fitted and close to 1 means perfectly fitted mostly 0.5 is the threshold 
value considered for this.

4) Adjusted R^2 Metric -
This metric came into picture when we observed that with increased number of attributes no-matter whether it is important or not the r-squared 
value tends to increase with every new feature added which can lead to overfitting condition of the model.
Adjusted r-square only tends to increase if the new column added helps in better model performance else it decreases.

-------------------------------------------------------------------------------------------------------------------------------------------------

HYPOTHESIS Testing,P-value,Z-Test,T-TEST,CHI-SQUARE TEST, ANOVA TEST,Type 1 and Type 2 Errors - 
------------------------------------------------------------------------------------------------

Hypothesis testing ??
--> is a statistical method that is used in making statistical decisions using experimental data. 
	Hypothesis Testing is basically an assumption that we make about the population parameter/feature for our given data set.

General Steps for Hypothesis Testing -
1) Make Initial Assumption
2) Collect Data
3) Gather Evidence to Reject or Not Reject NULL Hypothesis.

NULL and Alternative Hypothesis - 
---------------------------------
The Initial assumption which we make based on our data-set is the NULL Hypothesis.
Alternative Hypothesis is just opposite condition to Null Hypothesis.

TYPE1- ERROR:
-------------
When we reject the null hypothesis, although that hypothesis was true.
Type I error is denoted by alpha.


TYPE2 - ERROR:
---------------
When we accept the null hypothesis but it is false. Type II errors are denoted by beta.


Example: 
1) Lets make an Assumption that a Person is not Guilty or Innocent. (Null Hypothesis for us(H0)).
2) We will start collecting data/evidences to find whether the person is guilty or not and to accept/reject NULL hypothesis.
3) We gather Lot of Evidences to Accept/ Reject NULL Hypothesis.


TYPE-1 Error condition for above example - 
When we know somehow that Person is not Guilty and we have lack of data/evidences to accept NULL Hypothesis then we don't have any option other 
than to reject NULL Hypothesis and this leads to Type1 Error since knowing actually the Truth we are not able to accept the Truth due to lack of 
Data.

TYPE-2 Error condition for above example - 
When we know somehow that Person is actually Guilty and due to lack of evidences we are accepting the Null Hypothesis then this condition leads to 
type2 error where actually the person is criminal but due to Lack of Evidence we are not able to prove him guilty and thus he/she becomes Innocent.

 
P-VALUE (Significance Value):
-----------------------------
The P value, or calculated probability, is the probability of finding the observed, or more extreme, results when the null hypothesis 
(H0) of a study question is true — the definition of 'extreme' depends on how the hypothesis is being tested.
If your P value is less than the chosen significance level then you reject the null hypothesis i.e. accept that your sample gives reasonable 
evidence to support the alternative hypothesis. It does NOT imply a “meaningful” or “important” difference; that is for you to decide when 
considering the real-world relevance of your result.

Example : you have a coin and you don’t know whether that is fair or tricky so let’s decide NULL and ALTERNATE Hypothesis.

H0 : a coin is a fair coin.
H1 : a coin is a tricky coin. and alpha = 5% or 0.05
Now let’s toss the coin and calculate p- value (probability value).
Toss a coin 1st time and result is tail- P-value = 50% (as head and tail have equal probability)
Toss a coin 2nd time and result is tail, now p-value = 50/2 = 25%
and similarly we Toss 6 consecutive time and got result as P-value = 1.5% but we set our significance level as 95% means 5% error rate we allow 
and here we see we are beyond that level i.e. our null- hypothesis does not hold good so we need to reject and propose that this coin is a tricky 
coin which it actually is.


 
T-Test:
--------
A t-test is a type of inferential statistic which is used to determine if there is a significant difference between the means of two groups which 
may be related in certain features. It is mostly used when the data sets, like the set of data recorded as outcome from flipping a coin a 100 times,
would follow a normal distribution and may have unknown variances. T test is used as a hypothesis testing tool, which allows testing of an 
assumption applicable to a population.

T-test has 2 types : 
1) One Sampled T-test 
2) Two-sampled T-test.


One sampled t-test : The One Sample t Test determines whether the sample mean is statistically different from a known or hypothesised population mean.
The One Sample t Test is a parametric test.
Example :- you have 10 ages and you are checking whether avg age is 30 or not. These tests are applied for Continuos/Numerical Variables.


Two sampled T-test :-The Independent Samples t Test or 2-sample t-test compares the means of two independent groups in order to determine whether 
there is statistical evidence that the associated population means are significantly different. The Independent Samples t Test is a parametric test.
This test is also known as: Independent t Test. These tests are applied for Continuos/Numerical Variables.
Example : is there any association between week1 and week2.


One-Sampled Proportion Test: is performed when we are considering the 1 Categorical parameter to accept/reject the null hypothesis.

Chi-Squared Test: is applied when two Categorical features are considered for accepting or rejecting the Null Hypothesis.

ANOVA Test: when we are considering One Numerical Column and 1 or multiple Categorical columns which have Category values of more than 2 types
            (Movie Ratings Column: 1 to 5) then ANOVA Test is applied to accept/reject the Null Hypothesis. The analysis of variance or ANOVA 
			is a statistical inference test that lets you compare multiple groups at the same time.
			
			

For better understanding refer Statquest, Krish Naik, Kaggle Blogs.
--------------------------------------------------------------------------------------------------------------------------------------------------


MULTI-COLLINEARITY:
-------------------
Multicollinearity occurs when two or more independent variables are highly correlated with one another in a regression model.
For example, height and weight, household income and water consumption, mileage and price of a car, study time and leisure time, etc.

There are various methods to detect Multicollinearity in which one is drawing correlation matrix,heatmaps by which if we have correlation between
two independent columns greater than 90% then one of the independent columns should be DROPPED to Solve this problem. This can be done practically
when we have lesser number of Features but for Data-sets which have more columns 100+ then it is advisable to apply Regularization techniques Lasso
and Ridge Regularization to penalize the coefficients and finally to resolve the Multicollinearity Issue.

NOTE: Multicollinearity can be detected via various methods. Most common one – VIF(Variable Inflation Factor)(Analytics Vidhya Blog).
---------------------------------------------------------------------------------------------------------------------------------------------------


HYPERPARAMETER TUNING/OPTIMIZATION ?
-----------------------------------

What are Hyperparameters?
The parameters that the model has when build are known as hyper-parameters, i.e. the parameters that control the training/fitting process of the 
model. Parameters which define the model architecture are referred to as hyperparameters and thus this process of searching for the ideal model 
architecture is referred to as hyperparameter tuning.


In a ML there are two types of Parameters:
1) Model Parameters - These are the parameters that must be determined with the help of training data set. These are actually the fitted parameters.
					  Example - Coefficient and Slope of Linear Regression Model.
					  

2) Hyperparameters - These are adjustable parameters that must be tuned in order to obtain a model with best performance, accuracy.
					 Example - For a KNN Classifier (n_neighbors,metric, p) values which can be manipulated are hyper-parameters.
					 
					 
					 
					 
					 
There are many Hyperparameter optimization techniques but the most common are below - 
1)Grid Search - This approach methodically builds the model and evaluate it for each combination of algorithm parameters specified in a Grid 
				and then finally selecting the model which performs the best. It is actually a little bit more time consuming and computing 
				is expensive,can be exhaustive and inefficient some times.

2) Randomized Search - In this technique we provide a statistical distribution of values for each hyperparameter from which values may be randomly
					   sampled and selected.Drawbacks of this technique is unnecessary high variance and thus its just a luck to get better 
					   performance since this technique chooses randoms values and sometimes it may work sometimes not. Better to use BAYESIAN 
					   OPTIMIZATION(Allows for one experiment/evaluation results to be used for next iteration to improve sampling method for next 
					   experiment) technique which can produce better results than above both techniques. 
					   
					   
Library for Above both techniques -
from sklearn.model_selection import GridSearchCV

from sklearn.model_selection import RandomizedSearchCV

---------------------------------------------------------------------------------------------------------------------------------------------------

HOW TO RESOLVE IMBALANCED DATA-SET ISSUE? 
Imbalanced Data-sets usually have data which are more inclined towards one of the classes of a column if we are dealing with a classification
problem. 
Example if we take a Balance Scale Data-set available in UCI ML Repository this dataset is imbalanced and the target 
variable of this data-set has value counts like -

df['balance'].value_counts()
# 0    576		Imbalanced Scale
# 1     49		Balanced Scale
# Name: balance, dtype: int64


Only 8% of the total observations are Balanced hence this type of DataSet is not good for making an accurate Classifier Model. Hence there are 
various ways to handle such type of usecases - 

1) UP-SAMPLE MINORITY Class - 
Upsampling/OVERSAMPLING is the process of randomly duplicating observations from the minority class in order to handle the Imbalances of the Dataset.
Steps: 1) Separate each class into new different DataFrames. 2) Resample the Minority Class with replacement same in number as for the Majority 
	   Class.  3) Finally combine the Upsampled Dataframe to the Majority Class newly created Dataframe in Step1.


Code to do Upsampling -
#Import utility for Resampling
from sklearn.utils import resample

# Separate majority and minority classes
df_majority = df[df.balance==0]
df_minority = df[df.balance==1]
 
# Upsample minority class
df_minority_upsampled = resample(df_minority, 
                                 replace=True,     # sample with replacement
                                 n_samples=576,    # to match majority class
                                 random_state=123) # reproducible results
 
# Combine majority class with upsampled minority class
df_upsampled = pd.concat([df_majority, df_minority_upsampled])
 
# Display new class counts
df_upsampled.balance.value_counts()
# 1    576
# 0    576
# Name: balance, dtype: int64

Now the new DataFrame has more observations than the original and the ratio of the two classes is now 1:1. Hence Imbalancing of the Dataset has 
been resolved.



2) DOWN-SAMPLE Majority Class:
Down-sampling involves randomly removing observations from the majority class to prevent its signal from dominating the learning algorithm.
The most common heuristic for doing so is resampling without replacement.
The process is similar to that of up-sampling. Here are the steps:

Steps: 1) Separate each class into new different DataFrames. 2) Resample the Majority Class WITHOUT replacement same in number as for the Minority 
	   Class.  3) Finally combine the Upsampled Dataframe to the Minority Class newly created Dataframe in Step1.
	   


# Separate majority and minority classes
df_majority = df[df.balance==0]
df_minority = df[df.balance==1]
 
# Downsample majority class
df_majority_downsampled = resample(df_majority, 
                                 replace=False,    # sample WITHOUT replacement(NOTE not replacing just removing to have lesser observations)
                                 n_samples=49,     # to match minority class count
                                 random_state=123) # reproducible results
 
# Combine minority class with downsampled majority class
df_downsampled = pd.concat([df_majority_downsampled, df_minority])
 
# Display new class counts
df_downsampled.balance.value_counts()
# 1    49
# 0    49
# Name: balance, dtype: int64


3) Change Evaluation/Performance Metrics sometimes based on scenarios ROC-AUC Score is determined to evaluate ,in other cases F1-Score is also
used for evaluating Model Performance hence it depends on different ML Tasks what we are performing to get appropriate Model Performance as 
required.

4) Collecting More Data can also resolve the Issue which will help in getting more number of Observations.

5) Use Tree-Based Algorithms
The final tactic we'll consider is using tree-based algorithms. Decision trees often perform well on imbalanced datasets because their hierarchical 
structure allows them to learn signals from both classes. In modern applied machine learning, tree ensembles 
(Random Forests, Gradient Boosted Trees, etc.) almost always outperform singular decision trees.

Well, tree ensembles have become very popular because they perform extremely well on many real-world problems. We certainly recommend them 
wholeheartedly. However: While these results are encouraging, the model could be overfit, so you should still evaluate your model on an unseen test
set before making the final decision.

Refer for more Info:
https://elitedatascience.com/imbalanced-classes

https://www.kdnuggets.com/2020/01/5-most-useful-techniques-handle-imbalanced-datasets.html

Krish Naik Video(DownSampling and OVERSAMPLING)

--------------------------------------------------------------------------------------------------------------------------------------------------


OUTLIER DETECTION/HANDLING?

What is an outlier?
An outlier is a data point in a data set that is distant from all other observations. A data point that lies outside the overall distribution 
of the dataset.

What are the criteria to identify an outlier?
1) Data point that falls outside of 1.5 times of an interquartile range above the 3rd quartile and below the 1st quartile
2) Data point that falls outside of 3 standard deviations. we can use a z score and if the z score falls outside of 2 standard deviation

What is the reason for an outlier to exists in a dataset?
1) Variability in the data
2) An experimental measurement error


What are the impacts of having outliers in a dataset?
1) It causes various problems during our statistical analysis.(Mean,Stddev changes in presence of Outlier but Median Does not.)
2) It may cause a significant impact on the mean and the standard deviation.
 
 
Various WAYS of finding the outlier?

1) Using Scatter Plots
2) Box plots
3) Using Z-Score
4) Using the IQR interquantile range.

Example of how to find outlier using Z score:
Formula for Z score = (Observation — Mean)/Standard Deviation

>>>
dataset = [11,10,12,14,12,15,14,13,15,102,12,14,17,19,107,10,13,12,14,12,108,12,11,14,13,15,10,15,12,10,14,13,15,10]

outliers=[]
def detect_outliers(data):    
    threshold=3
    mean = np.mean(data)
    std =np.std(data)    
    
    for i in data:
        z_score = (i - mean)/std 
        if np.abs(z_score) > threshold:
            outliers.append(i)
    return outliers

outlier_pt = detect_outliers(dataset)
outlier_pt

Output:
[102,107,108]



Using Inter Quartile Range:
25%- 75% values in a dataset

Steps
1. Arrange the data in increasing order
2. Calculate first Quartile(Q1) and third Quartile(Q3)
3. Find interquartile range (q3-q1)
4. Find lower bound q1*1.5
5. Find upper bound q3*1.5

Anything that lies outside of lower and upper bound is an outlier.
Removing Outliers can be done by filtering all those records which are above the threshold in case of Z-Score technique.

>>>
df = df[(z<3).all(axis = 1)] #Filtering and collecting all those records where z less than 3 and thus removing outliers.

-------------------------------------------------------------------------------------------------------------------------------------------------

Homoscedasticity -
It is a situation in which the error term(noise/random disturbance between dependent and independent variables) is same across all values of
independent variables. Error has Constant Variance.

In Linear Regression (Simple or Multiple) there are some error terms also present in the equation and hence few Assumptions are there for building
a model with Linear Regression - 
1) Linear Relationship between dependent and independent variable.
2) No Multicollinearity and Auto-Correlation
3) Homoscedasticity.

Note -> Violation of Homoscedasticity results in Heteroscedasticity .


Heteroscedasticity: where the error do not have a Constant Variance and it varies.
It occurs in dataset which have large range of min, max values where there are Outliers. Incorrect transformation of data is done to perform
Regression.


------------------------------------------------------------------------------------------------------------------------------------------------

PySpark --
---------





https://youtu.be/0rjlviOQlbc










